{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bdcf451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02977553  0.02695098 -0.03981876  0.03226086]\n",
      "r 38.0\n",
      "children [<__main__.MCTSNode object at 0x7f031fe1fdf0>, <__main__.MCTSNode object at 0x7f031fe1f9a0>]\n",
      "0.0\n",
      "0.0\n",
      "mv 0.0\n",
      "bst 0.0\n",
      "curr 38.0\n",
      "action 0\n",
      "r 33.0\n",
      "children [<__main__.MCTSNode object at 0x7f031fe1f5b0>, <__main__.MCTSNode object at 0x7f031fe1f130>]\n",
      "0.0\n",
      "33.0\n",
      "mv 33.0\n",
      "bst 33.0\n",
      "curr 33.0\n",
      "action 1\n",
      "r 19.0\n",
      "children [<__main__.MCTSNode object at 0x7f031fe1faf0>, <__main__.MCTSNode object at 0x7f031fe1f160>]\n",
      "52.0\n",
      "0.0\n",
      "mv 52.0\n",
      "bst 0.0\n",
      "curr 52.0\n",
      "action 0\n",
      "r 26.0\n",
      "children [<__main__.MCTSNode object at 0x7f03327cc550>, <__main__.MCTSNode object at 0x7f031fe1f9d0>]\n",
      "26.0\n",
      "52.0\n",
      "mv 52.0\n",
      "bst 26.0\n",
      "curr 78.0\n",
      "action 1\n",
      "r 18.0\n",
      "r 4.0\n",
      "children [<__main__.MCTSNode object at 0x7f031fb36b50>, <__main__.MCTSNode object at 0x7f031fb36d00>]\n",
      "55.0\n",
      "19.0\n",
      "mv 55.0\n",
      "bst 19.0\n",
      "curr 74.0\n",
      "action 0\n",
      "r 21.0\n",
      "children [<__main__.MCTSNode object at 0x7f031fb36f10>, <__main__.MCTSNode object at 0x7f031fb36ca0>]\n",
      "0.0\n",
      "76.0\n",
      "mv 76.0\n",
      "bst 0.0\n",
      "curr 76.0\n",
      "action 1\n",
      "r 33.0\n",
      "r 8.0\n",
      "children [<__main__.MCTSNode object at 0x7f031fb14fd0>, <__main__.MCTSNode object at 0x7f031fb230a0>]\n",
      "8.0\n",
      "109.0\n",
      "mv 109.0\n",
      "bst 109.0\n",
      "curr 117.0\n",
      "action 1\n",
      "r 14.0\n",
      "children [<__main__.MCTSNode object at 0x7f03240ca6a0>, <__main__.MCTSNode object at 0x7f03240ca730>]\n",
      "72.0\n",
      "51.0\n",
      "mv 72.0\n",
      "bst 51.0\n",
      "curr 123.0\n",
      "action 0\n",
      "r 22.0\n",
      "children [<__main__.MCTSNode object at 0x7f03240ae430>, <__main__.MCTSNode object at 0x7f03240ae100>]\n",
      "21.0\n",
      "73.0\n",
      "mv 73.0\n",
      "bst 21.0\n",
      "curr 94.0\n",
      "action 1\n",
      "r 32.0\n",
      "children [<__main__.MCTSNode object at 0x7f03242e7cd0>, <__main__.MCTSNode object at 0x7f03242e7a60>]\n",
      "73.0\n",
      "32.0\n",
      "mv 73.0\n",
      "bst 32.0\n",
      "curr 105.0\n",
      "action 0\n",
      "r 17.0\n",
      "children [<__main__.MCTSNode object at 0x7f03242b1160>, <__main__.MCTSNode object at 0x7f031fe1fa00>]\n",
      "51.0\n",
      "39.0\n",
      "mv 51.0\n",
      "bst 51.0\n",
      "curr 90.0\n",
      "action 0\n",
      "r 47.0\n",
      "children [<__main__.MCTSNode object at 0x7f03242b1610>, <__main__.MCTSNode object at 0x7f03242b16a0>]\n",
      "0.0\n",
      "98.0\n",
      "mv 98.0\n",
      "bst 0.0\n",
      "curr 98.0\n",
      "action 1\n",
      "r 15.0\n",
      "r 7.0\n",
      "children [<__main__.MCTSNode object at 0x7f0324139970>, <__main__.MCTSNode object at 0x7f0324139be0>]\n",
      "40.0\n",
      "80.0\n",
      "mv 80.0\n",
      "bst 80.0\n",
      "curr 120.0\n",
      "action 1\n",
      "r 18.0\n",
      "r 39.0\n",
      "children [<__main__.MCTSNode object at 0x7f03246148e0>, <__main__.MCTSNode object at 0x7f0324614970>]\n",
      "90.0\n",
      "47.0\n",
      "mv 90.0\n",
      "bst 47.0\n",
      "curr 137.0\n",
      "action 0\n",
      "r 21.0\n",
      "children [<__main__.MCTSNode object at 0x7f0324543070>, <__main__.MCTSNode object at 0x7f03244f4cd0>]\n",
      "36.0\n",
      "57.0\n",
      "mv 57.0\n",
      "bst 36.0\n",
      "curr 111.0\n",
      "action 1\n",
      "r 11.0\n",
      "children [<__main__.MCTSNode object at 0x7f032453dd90>, <__main__.MCTSNode object at 0x7f03244f4e50>]\n",
      "68.0\n",
      "0.0\n",
      "mv 68.0\n",
      "bst 68.0\n",
      "curr 68.0\n",
      "action 0\n",
      "r 13.0\n",
      "children [<__main__.MCTSNode object at 0x7f032453dbb0>, <__main__.MCTSNode object at 0x7f032453dd30>]\n",
      "11.0\n",
      "70.0\n",
      "mv 70.0\n",
      "bst 70.0\n",
      "curr 81.0\n",
      "action 1\n",
      "r 10.0\n",
      "r 30.0\n",
      "children [<__main__.MCTSNode object at 0x7f032453d820>, <__main__.MCTSNode object at 0x7f032453de80>]\n",
      "80.0\n",
      "30.0\n",
      "mv 80.0\n",
      "bst 30.0\n",
      "curr 110.0\n",
      "action 0\n",
      "r 10.0\n",
      "children [<__main__.MCTSNode object at 0x7f0324543670>, <__main__.MCTSNode object at 0x7f0324531070>]\n",
      "70.0\n",
      "20.0\n",
      "mv 70.0\n",
      "bst 20.0\n",
      "curr 90.0\n",
      "action 0\n",
      "r 12.0\n",
      "children [<__main__.MCTSNode object at 0x7f032454f1f0>, <__main__.MCTSNode object at 0x7f03245433a0>]\n",
      "82.0\n",
      "0.0\n",
      "mv 82.0\n",
      "bst 0.0\n",
      "curr 82.0\n",
      "action 0\n",
      "r 22.0\n",
      "children [<__main__.MCTSNode object at 0x7f03242ff550>, <__main__.MCTSNode object at 0x7f03242ff5e0>]\n",
      "0.0\n",
      "104.0\n",
      "mv 104.0\n",
      "bst 104.0\n",
      "curr 104.0\n",
      "action 1\n",
      "r 15.0\n",
      "children [<__main__.MCTSNode object at 0x7f031f799670>, <__main__.MCTSNode object at 0x7f031f799910>]\n",
      "15.0\n",
      "104.0\n",
      "mv 104.0\n",
      "bst 104.0\n",
      "curr 119.0\n",
      "action 1\n",
      "r 15.0\n",
      "children [<__main__.MCTSNode object at 0x7f031f6d1190>, <__main__.MCTSNode object at 0x7f031f7da640>]\n",
      "0.0\n",
      "101.0\n",
      "mv 101.0\n",
      "bst 0.0\n",
      "curr 119.0\n",
      "action 1\n",
      "r 32.0\n",
      "children [<__main__.MCTSNode object at 0x7f031f5c08b0>, <__main__.MCTSNode object at 0x7f031f6d1400>]\n",
      "34.0\n",
      "99.0\n",
      "mv 99.0\n",
      "bst 34.0\n",
      "curr 133.0\n",
      "action 1\n",
      "r 7.0\n",
      "children [<__main__.MCTSNode object at 0x7f031f5c0f40>, <__main__.MCTSNode object at 0x7f031f5c0c10>]\n",
      "39.0\n",
      "67.0\n",
      "mv 67.0\n",
      "bst 39.0\n",
      "curr 106.0\n",
      "action 1\n",
      "r 7.0\n",
      "children [<__main__.MCTSNode object at 0x7f031f5c9370>, <__main__.MCTSNode object at 0x7f031f5c9790>]\n",
      "0.0\n",
      "74.0\n",
      "mv 74.0\n",
      "bst 74.0\n",
      "curr 74.0\n",
      "action 1\n",
      "r 3.0\n",
      "children [<__main__.MCTSNode object at 0x7f031f5c9b80>, <__main__.MCTSNode object at 0x7f031f5c9e20>]\n",
      "70.0\n",
      "7.0\n",
      "mv 70.0\n",
      "bst 70.0\n",
      "curr 77.0\n",
      "action 0\n",
      "r 3.0\n",
      "r 21.0\n",
      "children [<__main__.MCTSNode object at 0x7f031f5d41c0>, <__main__.MCTSNode object at 0x7f031f5d4340>]\n",
      "10.0\n",
      "84.0\n",
      "mv 84.0\n",
      "bst 84.0\n",
      "curr 94.0\n",
      "action 1\n",
      "r 2.0\n",
      "children [<__main__.MCTSNode object at 0x7f031f56e100>, <__main__.MCTSNode object at 0x7f031f56ef10>]\n",
      "15.0\n",
      "71.0\n",
      "mv 71.0\n",
      "bst 15.0\n",
      "curr 86.0\n",
      "action 1\n",
      "r 1.0\n",
      "children [<__main__.MCTSNode object at 0x7f031f3267f0>, <__main__.MCTSNode object at 0x7f031f434100>]\n",
      "18.0\n",
      "54.0\n",
      "mv 54.0\n",
      "bst 54.0\n",
      "curr 72.0\n",
      "action 1\n",
      "done\n",
      "30.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import animation\n",
    "\n",
    "class MCTSNode:\n",
    "    def __init__(self, state, parent=None, action=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.action = action\n",
    "        self.visits = 0\n",
    "        self.wins = 0\n",
    "\n",
    "    def is_fully_expanded(self, action_space_size):\n",
    "        return len(self.children) == action_space_size\n",
    "\n",
    "    def best_child(self, exploration_param=1.414):\n",
    "        choices_weights = [\n",
    "            (child.wins / max(1, child.visits)) + exploration_param * math.sqrt(\n",
    "                math.log(max(1, self.visits)) / max(1, child.visits)\n",
    "            ) for child in self.children\n",
    "        ]\n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "\n",
    "    def most_visited_child(self):\n",
    "        return max(self.children, key=lambda child: child.visits)\n",
    "\n",
    "def discretize_state(state, bins):\n",
    "    \"\"\"\n",
    "    Discretize the continuous state into a discrete bin representation.\n",
    "    We create bins for each feature (position, velocity, angle, angular velocity).\n",
    "    \"\"\"\n",
    "    binned_state = []\n",
    "    for i in range(len(state)):\n",
    "#         print('state[i]', state[i])\n",
    "#         print('bin[i]', bins[i])\n",
    "        binned_state.append(np.digitize(state[i], bins[i]))\n",
    "    return tuple(binned_state)\n",
    "\n",
    "def create_bins():\n",
    "    \"\"\"\n",
    "    Create bins for each state variable.\n",
    "    CartPole has 4 continuous state variables:\n",
    "    [Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity].\n",
    "    We define the ranges and bins for each.\n",
    "    \"\"\"\n",
    "    bins = [\n",
    "        np.linspace(-4.8, 4.8, 10),  # Cart Position\n",
    "        np.linspace(-5.0, 5.0, 10),  # Cart Velocity\n",
    "        np.linspace(-0.418, 0.418, 10),  # Pole Angle\n",
    "        np.linspace(-5.0, 5.0, 10),  # Pole Angular Velocity\n",
    "    ]\n",
    "    return bins\n",
    "\n",
    "def rollout(env, bins):\n",
    "    \"\"\"\n",
    "    Simulate a random rollout from the current state.\n",
    "    \"\"\"\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Randomly pick an action (exploration)\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "def backpropagate(node, reward):\n",
    "    while node is not None:\n",
    "        node.visits += 1\n",
    "        node.wins += reward\n",
    "        node = node.parent\n",
    "\n",
    "def expand(node, env, action_space_size, bins):\n",
    "    \"\"\"\n",
    "    Expand the node by creating child nodes for each action.\n",
    "    \"\"\"\n",
    "    for action in range(action_space_size):\n",
    "        # Clone the environment and step with the current action\n",
    "        env_copy = gym.make('CartPole-v1')\n",
    "        env_copy.reset()\n",
    "        env_copy.env.state = env.env.state  # Copy the environment state\n",
    "        state, reward, done, _, _ = env_copy.step(action)\n",
    "\n",
    "        # Discretize the state to get the discrete representation\n",
    "        discrete_state = discretize_state(state, bins)\n",
    "\n",
    "        child_node = MCTSNode(state=discrete_state, parent=node, action=action)\n",
    "        node.children.append(child_node)\n",
    "\n",
    "def select(node, action_space_size):\n",
    "    \"\"\"\n",
    "    Traverse the tree by selecting the best child node based on the UCB1 algorithm.\n",
    "    \"\"\"\n",
    "    current_node = node\n",
    "    while current_node.is_fully_expanded(action_space_size):\n",
    "        current_node = current_node.best_child()\n",
    "    return current_node\n",
    "\n",
    "def mcts(env, state, current_node, simulations=1000):\n",
    "    env_copy = copy.deepcopy(env)\n",
    "    action_space_size = env.action_space.n\n",
    "\n",
    "    for _ in range(simulations):\n",
    "        # Step 1: Selection\n",
    "        selected_node = select(current_node, action_space_size)\n",
    "#         print('sn',selected_node)\n",
    "        \n",
    "        # Step 2: Expansion\n",
    "        expand(selected_node, env_copy, action_space_size, bins)\n",
    "        \n",
    "        # Step 3: Simulation\n",
    "        reward = rollout(env_copy, bins)\n",
    "        if reward !=0:\n",
    "            print('r', reward)\n",
    "        \n",
    "        # Step 4: Backpropagation\n",
    "        backpropagate(selected_node, reward)\n",
    "        \n",
    "    best_node = current_node.best_child(exploration_param=0)\n",
    "    print('children', current_node.children)\n",
    "    for c in current_node.children:\n",
    "        print(c.wins)\n",
    "    print('mv', current_node.most_visited_child().wins)\n",
    "    print('bst', current_node.best_child().wins)\n",
    "    print('curr', current_node.wins)\n",
    "    return best_node\n",
    "\n",
    "# Initialize the CartPole environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Run MCTS for 1000 simulations\n",
    "\n",
    "\n",
    "# Apply the best move (select action based on the best child)\n",
    "state = env.reset()[0]\n",
    "print(state)\n",
    "done = False\n",
    "tot_r = 0\n",
    "\n",
    "# Discretize the initial state\n",
    "bins = create_bins()\n",
    "discrete_state = discretize_state(state, bins)\n",
    "current_node = MCTSNode(discrete_state) # first will be root node\n",
    "\n",
    "while not done:\n",
    "    current_node = mcts(env, state, current_node, simulations=1000)\n",
    "    print('action', current_node.action)\n",
    "    state, reward, done, _, _ = env.step(current_node.action)\n",
    "    tot_r += reward\n",
    "    #     print(reward)\n",
    "    env.render()\n",
    "\n",
    "print('done')\n",
    "print(tot_r)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08451ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ac596c63",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'pygame.surface.Surface' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-967201aafeaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0mcurrent_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimulations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mtot_r\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-967201aafeaf>\u001b[0m in \u001b[0;36mmcts\u001b[0;34m(env, state, current_node, simulations)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmcts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimulations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0menv_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0maction_space_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_mbpo/lib/python3.9/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_mbpo/lib/python3.9/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_mbpo/lib/python3.9/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_mbpo/lib/python3.9/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_mbpo/lib/python3.9/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_mbpo/lib/python3.9/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_mbpo/lib/python3.9/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_mbpo/lib/python3.9/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_mbpo/lib/python3.9/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_mbpo/lib/python3.9/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_mbpo/lib/python3.9/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_mbpo/lib/python3.9/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_mbpo/lib/python3.9/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_mbpo/lib/python3.9/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_mbpo/lib/python3.9/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_mbpo/lib/python3.9/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_mbpo/lib/python3.9/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    159\u001b[0m                     \u001b[0mreductor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__reduce_ex__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mreductor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                         \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreductor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                         \u001b[0mreductor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__reduce__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle 'pygame.surface.Surface' object"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import animation\n",
    "from gym.wrappers import RecordVideo\n",
    "from IPython.display import Video\n",
    "import cv2\n",
    "\n",
    "class MCTSNode:\n",
    "    def __init__(self, state, parent=None, action=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.action = action\n",
    "        self.visits = 0\n",
    "        self.wins = 0\n",
    "\n",
    "    def is_fully_expanded(self, action_space_size):\n",
    "        return len(self.children) == action_space_size\n",
    "\n",
    "    def best_child(self, exploration_param=1.414):\n",
    "        choices_weights = [\n",
    "            (child.wins / max(1, child.visits)) + exploration_param * math.sqrt(\n",
    "                math.log(max(1, self.visits)) / max(1, child.visits)\n",
    "            ) for child in self.children\n",
    "        ]\n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "\n",
    "    def most_visited_child(self):\n",
    "        return max(self.children, key=lambda child: child.visits)\n",
    "\n",
    "def discretize_state(state, bins):\n",
    "    \"\"\"\n",
    "    Discretize the continuous state into a discrete bin representation.\n",
    "    We create bins for each feature (position, velocity, angle, angular velocity).\n",
    "    \"\"\"\n",
    "    binned_state = []\n",
    "    for i in range(len(state)):\n",
    "        binned_state.append(np.digitize(state[i], bins[i]))\n",
    "    return tuple(binned_state)\n",
    "\n",
    "def create_bins():\n",
    "    \"\"\"\n",
    "    Create bins for each state variable.\n",
    "    CartPole has 4 continuous state variables:\n",
    "    [Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity].\n",
    "    We define the ranges and bins for each.\n",
    "    \"\"\"\n",
    "    bins = [\n",
    "        np.linspace(-4.8, 4.8, 10),  # Cart Position\n",
    "        np.linspace(-5.0, 5.0, 10),  # Cart Velocity\n",
    "        np.linspace(-0.418, 0.418, 10),  # Pole Angle\n",
    "        np.linspace(-5.0, 5.0, 10),  # Pole Angular Velocity\n",
    "    ]\n",
    "    return bins\n",
    "\n",
    "def rollout(env, bins):\n",
    "    \"\"\"\n",
    "    Simulate a random rollout from the current state.\n",
    "    \"\"\"\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Randomly pick an action (exploration)\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "def backpropagate(node, reward):\n",
    "    while node is not None:\n",
    "        node.visits += 1\n",
    "        node.wins += reward\n",
    "        node = node.parent\n",
    "\n",
    "def expand(node, env, action_space_size, bins):\n",
    "    \"\"\"\n",
    "    Expand the node by creating child nodes for each action.\n",
    "    \"\"\"\n",
    "    for action in range(action_space_size):\n",
    "        # Clone the environment and step with the current action\n",
    "        env_copy = gym.make('CartPole-v1')\n",
    "        env_copy.reset()\n",
    "        env_copy.env.state = env.env.state  # Copy the environment state\n",
    "        state, reward, done, _, _ = env_copy.step(action)\n",
    "\n",
    "        # Discretize the state to get the discrete representation\n",
    "        discrete_state = discretize_state(state, bins)\n",
    "\n",
    "        child_node = MCTSNode(state=discrete_state, parent=node, action=action)\n",
    "        node.children.append(child_node)\n",
    "\n",
    "def select(node, action_space_size):\n",
    "    \"\"\"\n",
    "    Traverse the tree by selecting the best child node based on the UCB1 algorithm.\n",
    "    \"\"\"\n",
    "    current_node = node\n",
    "    while current_node.is_fully_expanded(action_space_size):\n",
    "        current_node = current_node.best_child()\n",
    "    return current_node\n",
    "\n",
    "def mcts(env, state, current_node, simulations=1000):\n",
    "    env_copy = copy.deepcopy(env)\n",
    "    action_space_size = env.action_space.n\n",
    "\n",
    "    for _ in range(simulations):\n",
    "        # Step 1: Selection\n",
    "        selected_node = select(current_node, action_space_size)\n",
    "#         print('sn',selected_node)\n",
    "        \n",
    "        # Step 2: Expansion\n",
    "        expand(selected_node, env_copy, action_space_size, bins)\n",
    "        \n",
    "        # Step 3: Simulation\n",
    "        reward = rollout(env_copy, bins)\n",
    "        if reward !=0:\n",
    "            print('r', reward)\n",
    "        \n",
    "        # Step 4: Backpropagation\n",
    "        backpropagate(selected_node, reward)\n",
    "        \n",
    "    best_node = current_node.best_child(exploration_param=0)\n",
    "    return best_node\n",
    "\n",
    "def show_frame(frame):\n",
    "    plt.imshow(frame)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Initialize the CartPole environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "# Define video recording parameters\n",
    "video_path = 'cartpole_mcts.mp4'\n",
    "frame_width = env.render().shape[1]\n",
    "frame_height = env.render().shape[0]\n",
    "fps = 30  # Set the frames per second\n",
    "\n",
    "# Create a video writer object using OpenCV\n",
    "video_writer = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Apply the best move (select action based on the best child)\n",
    "env.reset()\n",
    "done = False\n",
    "tot_r = 0\n",
    "\n",
    "# Discretize the initial state\n",
    "bins = create_bins()\n",
    "discrete_state = discretize_state(state, bins)\n",
    "current_node = MCTSNode(discrete_state) # first will be root node\n",
    "\n",
    "while not done:\n",
    "    current_node = mcts(env, state, current_node, simulations=1000)\n",
    "    state, reward, done, _, _ = env.step(1)\n",
    "    tot_r += reward\n",
    "\n",
    "\n",
    "\n",
    "print('done')\n",
    "print(tot_r)\n",
    "env.close()\n",
    "\n",
    "\n",
    "# Release the video writer\n",
    "video_writer.release()\n",
    "\n",
    "# Display the recorded video in the notebook\n",
    "Video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cb5798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bfde76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d48ed88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46d6708f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action 0\n",
      "action 1\n",
      "action 1\n",
      "action 0\n",
      "action 1\n",
      "action 0\n",
      "action 1\n",
      "action 1\n",
      "action 1\n",
      "action 1\n",
      "action 1\n",
      "action 1\n",
      "action 1\n",
      "action 1\n",
      "action 0\n",
      "done\n",
      "15.0\n"
     ]
    }
   ],
   "source": [
    "#100 bins\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import animation\n",
    "\n",
    "class MCTSNode:\n",
    "    def __init__(self, state, parent=None, action=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.action = action\n",
    "        self.visits = 0\n",
    "        self.wins = 0\n",
    "\n",
    "    def is_fully_expanded(self, action_space_size):\n",
    "        return len(self.children) == action_space_size\n",
    "\n",
    "    def best_child(self, exploration_param=1.414):\n",
    "        choices_weights = [\n",
    "            (child.wins / max(1, child.visits)) + exploration_param * math.sqrt(\n",
    "                math.log(max(1, self.visits)) / max(1, child.visits)\n",
    "            ) for child in self.children\n",
    "        ]\n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "\n",
    "    def most_visited_child(self):\n",
    "        return max(self.children, key=lambda child: child.visits)\n",
    "\n",
    "def discretize_state(state, bins):\n",
    "    \"\"\"\n",
    "    Discretize the continuous state into a discrete bin representation.\n",
    "    We create bins for each feature (position, velocity, angle, angular velocity).\n",
    "    \"\"\"\n",
    "    binned_state = []\n",
    "    for i in range(len(state)):\n",
    "#         print('state[i]', state[i])\n",
    "#         print('bin[i]', bins[i])\n",
    "        binned_state.append(np.digitize(state[i], bins[i]))\n",
    "    return tuple(binned_state)\n",
    "\n",
    "def create_bins():\n",
    "    \"\"\"\n",
    "    Create bins for each state variable.\n",
    "    CartPole has 4 continuous state variables:\n",
    "    [Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity].\n",
    "    We define the ranges and bins for each.\n",
    "    \"\"\"\n",
    "    bins = [\n",
    "        np.linspace(-4.8, 4.8, 100),  # Cart Position\n",
    "        np.linspace(-5.0, 5.0, 100),  # Cart Velocity\n",
    "        np.linspace(-0.418, 0.418, 100),  # Pole Angle\n",
    "        np.linspace(-5.0, 5.0, 100),  # Pole Angular Velocity\n",
    "    ]\n",
    "    return bins\n",
    "\n",
    "def rollout(env, bins):\n",
    "    \"\"\"\n",
    "    Simulate a random rollout from the current state.\n",
    "    \"\"\"\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Randomly pick an action (exploration)\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "def backpropagate(node, reward):\n",
    "    while node is not None:\n",
    "        node.visits += 1\n",
    "        node.wins += reward\n",
    "        node = node.parent\n",
    "\n",
    "def expand(node, env, action_space_size, bins):\n",
    "    \"\"\"\n",
    "    Expand the node by creating child nodes for each action.\n",
    "    \"\"\"\n",
    "    for action in range(action_space_size):\n",
    "        # Clone the environment and step with the current action\n",
    "        env_copy = gym.make('CartPole-v1')\n",
    "        env_copy.reset()\n",
    "        env_copy.env.state = env.env.state  # Copy the environment state\n",
    "        state, reward, done, _, _ = env_copy.step(action)\n",
    "\n",
    "        # Discretize the state to get the discrete representation\n",
    "        discrete_state = discretize_state(state, bins)\n",
    "\n",
    "        child_node = MCTSNode(state=discrete_state, parent=node, action=action)\n",
    "        node.children.append(child_node)\n",
    "\n",
    "def select(node, action_space_size):\n",
    "    \"\"\"\n",
    "    Traverse the tree by selecting the best child node based on the UCB1 algorithm.\n",
    "    \"\"\"\n",
    "    current_node = node\n",
    "    while current_node.is_fully_expanded(action_space_size):\n",
    "        current_node = current_node.best_child()\n",
    "    return current_node\n",
    "\n",
    "def mcts(env, state, current_node, simulations=1000):\n",
    "    env_copy = copy.deepcopy(env)\n",
    "    action_space_size = env.action_space.n\n",
    "\n",
    "    for _ in range(simulations):\n",
    "        # Step 1: Selection\n",
    "        selected_node = select(current_node, action_space_size)\n",
    "#         print('sn',selected_node)\n",
    "        \n",
    "        # Step 2: Expansion\n",
    "        expand(selected_node, env_copy, action_space_size, bins)\n",
    "        \n",
    "        # Step 3: Simulation\n",
    "        reward = rollout(env_copy, bins)\n",
    "        \n",
    "        # Step 4: Backpropagation\n",
    "        backpropagate(selected_node, reward)\n",
    "        \n",
    "    best_node = current_node.best_child(exploration_param=0)\n",
    "\n",
    "    return best_node\n",
    "\n",
    "# Initialize the CartPole environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Run MCTS for 1000 simulations\n",
    "\n",
    "\n",
    "# Apply the best move (select action based on the best child)\n",
    "state = env.reset()[0]\n",
    "done = False\n",
    "tot_r = 0\n",
    "\n",
    "# Discretize the initial state\n",
    "bins = create_bins()\n",
    "discrete_state = discretize_state(state, bins)\n",
    "current_node = MCTSNode(discrete_state) # first will be root node\n",
    "\n",
    "while not done:\n",
    "    current_node = mcts(env, state, current_node, simulations=1000)\n",
    "    print('action', current_node.action)\n",
    "    state, reward, done, _, _ = env.step(current_node.action)\n",
    "    tot_r += reward\n",
    "    env.render()\n",
    "\n",
    "print('done')\n",
    "print(tot_r)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4627b4c5",
   "metadata": {},
   "source": [
    "# Use MCTS above to generate data, then train a NN to that data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad4bb43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/envs/n_mbpo/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.state to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.state` for environment variables or `env.get_wrapper_attr('state')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/james/miniconda3/envs/n_mbpo/lib/python3.9/site-packages/gymnasium/envs/classic_control/cartpole.py:180: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/james/miniconda3/envs/n_mbpo/lib/python3.9/site-packages/gymnasium/envs/classic_control/cartpole.py:215: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of eps 0\n",
      "end of eps 1\n",
      "end of eps 2\n",
      "end of eps 3\n",
      "end of eps 4\n",
      "end of eps 5\n",
      "end of eps 6\n",
      "end of eps 7\n",
      "end of eps 8\n",
      "end of eps 9\n",
      "end of eps 10\n",
      "end of eps 11\n",
      "end of eps 12\n",
      "end of eps 13\n",
      "end of eps 14\n",
      "end of eps 15\n",
      "end of eps 16\n",
      "end of eps 17\n",
      "end of eps 18\n",
      "end of eps 19\n",
      "end of eps 20\n",
      "end of eps 21\n",
      "end of eps 22\n",
      "end of eps 23\n",
      "end of eps 24\n",
      "end of eps 25\n",
      "end of eps 26\n",
      "end of eps 27\n",
      "end of eps 28\n",
      "end of eps 29\n",
      "end of eps 30\n",
      "end of eps 31\n",
      "end of eps 32\n",
      "end of eps 33\n",
      "end of eps 34\n",
      "end of eps 35\n",
      "end of eps 36\n",
      "end of eps 37\n",
      "end of eps 38\n",
      "end of eps 39\n",
      "end of eps 40\n",
      "end of eps 41\n",
      "end of eps 42\n",
      "end of eps 43\n",
      "end of eps 44\n",
      "end of eps 45\n",
      "end of eps 46\n",
      "end of eps 47\n",
      "end of eps 48\n",
      "end of eps 49\n",
      "end of eps 50\n",
      "end of eps 51\n",
      "end of eps 52\n",
      "end of eps 53\n",
      "end of eps 54\n",
      "end of eps 55\n",
      "end of eps 56\n",
      "end of eps 57\n",
      "end of eps 58\n",
      "end of eps 59\n",
      "end of eps 60\n",
      "end of eps 61\n",
      "end of eps 62\n",
      "end of eps 63\n",
      "end of eps 64\n",
      "end of eps 65\n",
      "end of eps 66\n",
      "end of eps 67\n",
      "end of eps 68\n",
      "end of eps 69\n",
      "end of eps 70\n",
      "end of eps 71\n",
      "end of eps 72\n",
      "end of eps 73\n",
      "end of eps 74\n",
      "end of eps 75\n",
      "end of eps 76\n",
      "end of eps 77\n",
      "end of eps 78\n",
      "end of eps 79\n",
      "end of eps 80\n",
      "end of eps 81\n",
      "end of eps 82\n",
      "end of eps 83\n",
      "end of eps 84\n",
      "end of eps 85\n",
      "end of eps 86\n",
      "end of eps 87\n",
      "end of eps 88\n",
      "end of eps 89\n",
      "end of eps 90\n",
      "end of eps 91\n",
      "end of eps 92\n",
      "end of eps 93\n",
      "end of eps 94\n",
      "end of eps 95\n",
      "end of eps 96\n",
      "end of eps 97\n",
      "end of eps 98\n",
      "end of eps 99\n",
      "end of eps 100\n",
      "end of eps 101\n",
      "end of eps 102\n",
      "end of eps 103\n",
      "end of eps 104\n",
      "end of eps 105\n",
      "end of eps 106\n",
      "end of eps 107\n",
      "end of eps 108\n",
      "end of eps 109\n",
      "end of eps 110\n",
      "end of eps 111\n",
      "end of eps 112\n",
      "end of eps 113\n",
      "end of eps 114\n",
      "end of eps 115\n",
      "end of eps 116\n",
      "end of eps 117\n",
      "end of eps 118\n",
      "end of eps 119\n",
      "end of eps 120\n",
      "end of eps 121\n",
      "end of eps 122\n",
      "end of eps 123\n",
      "end of eps 124\n",
      "end of eps 125\n",
      "end of eps 126\n",
      "end of eps 127\n",
      "end of eps 128\n",
      "end of eps 129\n",
      "end of eps 130\n",
      "end of eps 131\n",
      "end of eps 132\n",
      "end of eps 133\n",
      "end of eps 134\n",
      "end of eps 135\n",
      "end of eps 136\n",
      "end of eps 137\n",
      "end of eps 138\n",
      "end of eps 139\n",
      "end of eps 140\n",
      "end of eps 141\n",
      "end of eps 142\n",
      "end of eps 143\n",
      "end of eps 144\n",
      "end of eps 145\n",
      "end of eps 146\n",
      "end of eps 147\n",
      "end of eps 148\n",
      "end of eps 149\n",
      "end of eps 150\n",
      "end of eps 151\n",
      "end of eps 152\n",
      "end of eps 153\n",
      "end of eps 154\n",
      "end of eps 155\n",
      "end of eps 156\n",
      "end of eps 157\n",
      "end of eps 158\n",
      "end of eps 159\n",
      "end of eps 160\n",
      "end of eps 161\n",
      "end of eps 162\n",
      "end of eps 163\n",
      "end of eps 164\n",
      "end of eps 165\n",
      "end of eps 166\n",
      "end of eps 167\n",
      "end of eps 168\n",
      "end of eps 169\n",
      "end of eps 170\n",
      "end of eps 171\n",
      "end of eps 172\n",
      "end of eps 173\n",
      "end of eps 174\n",
      "end of eps 175\n",
      "end of eps 176\n",
      "end of eps 177\n",
      "end of eps 178\n",
      "end of eps 179\n",
      "end of eps 180\n",
      "end of eps 181\n",
      "end of eps 182\n",
      "end of eps 183\n",
      "end of eps 184\n",
      "end of eps 185\n",
      "end of eps 186\n",
      "end of eps 187\n",
      "end of eps 188\n",
      "end of eps 189\n",
      "end of eps 190\n",
      "end of eps 191\n",
      "end of eps 192\n",
      "end of eps 193\n",
      "end of eps 194\n",
      "end of eps 195\n",
      "end of eps 196\n",
      "end of eps 197\n",
      "end of eps 198\n",
      "end of eps 199\n",
      "end of eps 200\n",
      "end of eps 201\n",
      "end of eps 202\n",
      "end of eps 203\n",
      "end of eps 204\n",
      "end of eps 205\n",
      "end of eps 206\n",
      "end of eps 207\n",
      "end of eps 208\n",
      "end of eps 209\n",
      "end of eps 210\n",
      "end of eps 211\n",
      "end of eps 212\n",
      "end of eps 213\n",
      "end of eps 214\n",
      "end of eps 215\n",
      "end of eps 216\n",
      "end of eps 217\n",
      "end of eps 218\n",
      "end of eps 219\n",
      "end of eps 220\n",
      "end of eps 221\n",
      "end of eps 222\n",
      "end of eps 223\n",
      "end of eps 224\n",
      "end of eps 225\n",
      "end of eps 226\n",
      "end of eps 227\n",
      "end of eps 228\n",
      "end of eps 229\n",
      "end of eps 230\n",
      "end of eps 231\n",
      "end of eps 232\n",
      "end of eps 233\n",
      "end of eps 234\n",
      "end of eps 235\n",
      "end of eps 236\n",
      "end of eps 237\n",
      "end of eps 238\n",
      "end of eps 239\n",
      "end of eps 240\n",
      "end of eps 241\n",
      "end of eps 242\n",
      "end of eps 243\n",
      "end of eps 244\n",
      "end of eps 245\n",
      "end of eps 246\n",
      "end of eps 247\n",
      "end of eps 248\n",
      "end of eps 249\n",
      "end of eps 250\n",
      "end of eps 251\n",
      "end of eps 252\n",
      "end of eps 253\n",
      "end of eps 254\n",
      "end of eps 255\n",
      "end of eps 256\n",
      "end of eps 257\n",
      "end of eps 258\n",
      "end of eps 259\n",
      "end of eps 260\n",
      "end of eps 261\n",
      "end of eps 262\n",
      "end of eps 263\n",
      "end of eps 264\n",
      "end of eps 265\n",
      "end of eps 266\n",
      "end of eps 267\n",
      "end of eps 268\n",
      "end of eps 269\n",
      "end of eps 270\n",
      "end of eps 271\n",
      "end of eps 272\n",
      "end of eps 273\n",
      "end of eps 274\n",
      "end of eps 275\n",
      "end of eps 276\n",
      "end of eps 277\n",
      "end of eps 278\n",
      "end of eps 279\n",
      "end of eps 280\n",
      "end of eps 281\n",
      "end of eps 282\n",
      "end of eps 283\n",
      "end of eps 284\n",
      "end of eps 285\n",
      "end of eps 286\n",
      "end of eps 287\n",
      "end of eps 288\n",
      "end of eps 289\n",
      "end of eps 290\n",
      "end of eps 291\n",
      "end of eps 292\n",
      "end of eps 293\n",
      "end of eps 294\n",
      "end of eps 295\n",
      "end of eps 296\n",
      "end of eps 297\n",
      "end of eps 298\n",
      "end of eps 299\n",
      "end of eps 300\n",
      "end of eps 301\n",
      "end of eps 302\n",
      "end of eps 303\n",
      "end of eps 304\n",
      "end of eps 305\n",
      "end of eps 306\n",
      "end of eps 307\n",
      "end of eps 308\n",
      "end of eps 309\n",
      "end of eps 310\n",
      "end of eps 311\n",
      "end of eps 312\n",
      "end of eps 313\n",
      "end of eps 314\n",
      "end of eps 315\n",
      "end of eps 316\n",
      "end of eps 317\n",
      "end of eps 318\n",
      "end of eps 319\n",
      "end of eps 320\n",
      "end of eps 321\n",
      "end of eps 322\n",
      "end of eps 323\n",
      "end of eps 324\n",
      "end of eps 325\n",
      "end of eps 326\n",
      "end of eps 327\n",
      "end of eps 328\n",
      "end of eps 329\n",
      "end of eps 330\n",
      "end of eps 331\n",
      "end of eps 332\n",
      "end of eps 333\n",
      "end of eps 334\n",
      "end of eps 335\n",
      "end of eps 336\n",
      "end of eps 337\n",
      "end of eps 338\n",
      "end of eps 339\n",
      "end of eps 340\n",
      "end of eps 341\n",
      "end of eps 342\n",
      "end of eps 343\n",
      "end of eps 344\n",
      "end of eps 345\n",
      "end of eps 346\n",
      "end of eps 347\n",
      "end of eps 348\n",
      "end of eps 349\n",
      "end of eps 350\n",
      "end of eps 351\n",
      "end of eps 352\n",
      "end of eps 353\n",
      "end of eps 354\n",
      "end of eps 355\n",
      "end of eps 356\n",
      "end of eps 357\n",
      "end of eps 358\n",
      "end of eps 359\n",
      "end of eps 360\n",
      "end of eps 361\n",
      "end of eps 362\n",
      "end of eps 363\n",
      "end of eps 364\n",
      "end of eps 365\n",
      "end of eps 366\n",
      "end of eps 367\n",
      "end of eps 368\n",
      "end of eps 369\n",
      "end of eps 370\n",
      "end of eps 371\n",
      "end of eps 372\n",
      "end of eps 373\n",
      "end of eps 374\n",
      "end of eps 375\n",
      "end of eps 376\n",
      "end of eps 377\n",
      "end of eps 378\n",
      "end of eps 379\n",
      "end of eps 380\n",
      "end of eps 381\n",
      "end of eps 382\n",
      "end of eps 383\n",
      "end of eps 384\n",
      "end of eps 385\n",
      "end of eps 386\n",
      "end of eps 387\n",
      "end of eps 388\n",
      "end of eps 389\n",
      "end of eps 390\n",
      "end of eps 391\n",
      "end of eps 392\n",
      "end of eps 393\n",
      "end of eps 394\n",
      "end of eps 395\n",
      "end of eps 396\n",
      "end of eps 397\n",
      "end of eps 398\n",
      "end of eps 399\n",
      "end of eps 400\n",
      "end of eps 401\n",
      "end of eps 402\n",
      "end of eps 403\n",
      "end of eps 404\n",
      "end of eps 405\n",
      "end of eps 406\n",
      "end of eps 407\n",
      "end of eps 408\n",
      "end of eps 409\n",
      "done\n",
      "RB len 10000\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "import gymnasium as gym\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import animation\n",
    "\n",
    "class MCTSNode:\n",
    "    def __init__(self, state, parent=None, action=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.action = action\n",
    "        self.visits = 0\n",
    "        self.wins = 0\n",
    "\n",
    "    def is_fully_expanded(self, action_space_size):\n",
    "        return len(self.children) == action_space_size\n",
    "\n",
    "    def best_child(self, exploration_param=1.414):\n",
    "        choices_weights = [\n",
    "            (child.wins / max(1, child.visits)) + exploration_param * math.sqrt(\n",
    "                math.log(max(1, self.visits)) / max(1, child.visits)\n",
    "            ) for child in self.children\n",
    "        ]\n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "\n",
    "    def most_visited_child(self):\n",
    "        return max(self.children, key=lambda child: child.visits)\n",
    "\n",
    "def discretize_state(state, bins):\n",
    "    \"\"\"\n",
    "    Discretize the continuous state into a discrete bin representation.\n",
    "    We create bins for each feature (position, velocity, angle, angular velocity).\n",
    "    \"\"\"\n",
    "    binned_state = []\n",
    "    for i in range(len(state)):\n",
    "        binned_state.append(np.digitize(state[i], bins[i]))\n",
    "    return tuple(binned_state)\n",
    "\n",
    "def create_bins():\n",
    "    \"\"\"\n",
    "    Create bins for each state variable.\n",
    "    CartPole has 4 continuous state variables:\n",
    "    [Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity].\n",
    "    We define the ranges and bins for each.\n",
    "    \"\"\"\n",
    "    bins = [\n",
    "        np.linspace(-4.8, 4.8, 100),  # Cart Position\n",
    "        np.linspace(-5.0, 5.0, 100),  # Cart Velocity\n",
    "        np.linspace(-0.418, 0.418, 100),  # Pole Angle\n",
    "        np.linspace(-5.0, 5.0, 100),  # Pole Angular Velocity\n",
    "    ]\n",
    "    return bins\n",
    "\n",
    "def rollout(env, bins):\n",
    "    \"\"\"\n",
    "    Simulate a random rollout from the current state.\n",
    "    \"\"\"\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Randomly pick an action (exploration)\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "def backpropagate(node, reward):\n",
    "    while node is not None:\n",
    "        node.visits += 1\n",
    "        node.wins += reward\n",
    "        node = node.parent\n",
    "\n",
    "def expand(node, env, action_space_size, bins):\n",
    "    \"\"\"\n",
    "    Expand the node by creating child nodes for each action.\n",
    "    \"\"\"\n",
    "    for action in range(action_space_size):\n",
    "        # Clone the environment and step with the current action\n",
    "        env_copy = gym.make('CartPole-v1')\n",
    "        env_copy.reset()\n",
    "        env_copy.env.state = env.env.state  # Copy the environment state\n",
    "        state, reward, done, _, _ = env_copy.step(action)\n",
    "\n",
    "        # Discretize the state to get the discrete representation\n",
    "        discrete_state = discretize_state(state, bins)\n",
    "\n",
    "        child_node = MCTSNode(state=discrete_state, parent=node, action=action)\n",
    "        node.children.append(child_node)\n",
    "\n",
    "def select(node, action_space_size):\n",
    "    \"\"\"\n",
    "    Traverse the tree by selecting the best child node based on the UCB1 algorithm.\n",
    "    \"\"\"\n",
    "    current_node = node\n",
    "    while current_node.is_fully_expanded(action_space_size):\n",
    "        current_node = current_node.best_child()\n",
    "    return current_node\n",
    "\n",
    "def mcts(env, state, current_node, simulations=1000):\n",
    "    env_copy = copy.deepcopy(env)\n",
    "    action_space_size = env.action_space.n\n",
    "\n",
    "    for _ in range(simulations):\n",
    "        # Step 1: Selection\n",
    "        selected_node = select(current_node, action_space_size)\n",
    "#         print('sn',selected_node)\n",
    "        \n",
    "        # Step 2: Expansion\n",
    "        expand(selected_node, env_copy, action_space_size, bins)\n",
    "        \n",
    "        # Step 3: Simulation\n",
    "        reward = rollout(env_copy, bins)\n",
    "        \n",
    "        # Step 4: Backpropagation\n",
    "        backpropagate(selected_node, reward)\n",
    "        \n",
    "    best_node = current_node.best_child(exploration_param=0)\n",
    "\n",
    "    return best_node\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in idx])\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def extract_data(self):\n",
    "        states, actions, rewards, next_states, dones = zip(*self.buffer)\n",
    "#         X = np.hstack((np.array(next_states), np.array(states), np.array(rewards).reshape(-1, 1), np.array(dones).reshape(-1, 1)))\n",
    "        # we only want state for X for now\n",
    "        X = np.array(states)\n",
    "        Y = np.array(actions)\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the CartPole environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "\n",
    "for i in range(410):\n",
    "\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    tot_r = 0\n",
    "\n",
    "    # Discretize the initial state\n",
    "    bins = create_bins()\n",
    "    discrete_state = discretize_state(state, bins)\n",
    "    current_node = MCTSNode(discrete_state) # first will be root node\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        current_node = mcts(env, state, current_node, simulations=1000)\n",
    "#         print('action', current_node.action)\n",
    "        next_state, reward, done, _, _ = env.step(current_node.action)\n",
    "        replay_buffer.add(state, current_node.action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        tot_r += reward\n",
    "        env.render()\n",
    "    print('end of eps', i)\n",
    "#     print(len(replay_buffer))\n",
    "\n",
    "print('done')\n",
    "print('RB len', len(replay_buffer))\n",
    "env.close()\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the replay buffer to a file\n",
    "with open('replay_buffer.pkl', 'wb') as f:\n",
    "    pickle.dump(replay_buffer, f)\n",
    "\n",
    "# Load the replay buffer from the file\n",
    "with open('replay_buffer.pkl', 'rb') as f:\n",
    "    rb = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8602af64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02710983 -0.5409558   0.01723253  0.73685557]\n",
      "4\n",
      "Epoch [10/100], Loss: 0.6902\n",
      "Epoch [20/100], Loss: 0.6888\n",
      "Epoch [30/100], Loss: 0.6875\n",
      "Epoch [40/100], Loss: 0.6860\n",
      "Epoch [50/100], Loss: 0.6844\n",
      "Epoch [60/100], Loss: 0.6825\n",
      "Epoch [70/100], Loss: 0.6805\n",
      "Epoch [80/100], Loss: 0.6784\n",
      "Epoch [90/100], Loss: 0.6763\n",
      "Epoch [100/100], Loss: 0.6744\n",
      "Test Accuracy: 0.5495\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Sample data from replay buffer\n",
    "X, Y = replay_buffer.extract_data()\n",
    "print(X[1])\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = torch.tensor(Y, dtype=torch.long)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "input_dim = X.shape[1] # lets just take state for now\n",
    "# input_dim = 4\n",
    "print(input_dim)\n",
    "output_dim = len(np.unique(Y))\n",
    "model = NeuralNetwork(input_dim, output_dim)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 100\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, Y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "    accuracy = (predicted == Y_test).float().mean()\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f5316c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 50.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Create the neural network\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "\n",
    "# Function to select an action using the neural network\n",
    "def select_action(state, model):\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        action_scores = model(state)\n",
    "    return torch.argmax(action_scores, dim=1).item()\n",
    "\n",
    "total_reward = 0\n",
    "state = env.reset()[0]\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = select_action(state, model)\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    env.render()\n",
    "\n",
    "print(\"Total Reward:\", total_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e3aaa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 24.0\n"
     ]
    }
   ],
   "source": [
    "#random agent\n",
    "import gymnasium as gym\n",
    "\n",
    "# Initialize the CartPole environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Reset the environment to get the initial state\n",
    "state = env.reset()\n",
    "\n",
    "total_reward = 0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # Sample a random action from the action space\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Apply the action to the environment\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    \n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "    \n",
    "    # Render the environment\n",
    "    env.render()\n",
    "\n",
    "print(\"Total Reward:\", total_reward)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470d58ff",
   "metadata": {},
   "source": [
    "#### While the MBRL agent is better than the random agent, in true MBRL, we want to learn the system dynamics. So X should be S,A; and Y should be S', R, T. and then you take the output with the highest R then selet that action to take.\n",
    "Question: will this work? Since CP gives 1 reward for every state where we are alive, but doesn't account for if that state is good or bad. Or is the reward factored into the MCTS backpropagation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3451e487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:n_mbpo] *",
   "language": "python",
   "name": "conda-env-n_mbpo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
