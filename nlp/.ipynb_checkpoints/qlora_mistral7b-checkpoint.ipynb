{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb7b457a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /home/james/miniconda3/envs/n_llm_rag/bin/python -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-yxqeo1t_/html2text_9c8fd82e225d460cb07418caba7ac97d/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-yxqeo1t_/html2text_9c8fd82e225d460cb07418caba7ac97d/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-0yae35rq\n",
      "         cwd: /tmp/pip-install-yxqeo1t_/html2text_9c8fd82e225d460cb07418caba7ac97d/\n",
      "    Complete output (11 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages/setuptools/__init__.py\", line 18, in <module>\n",
      "        from setuptools.dist import Distribution\n",
      "      File \"/home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages/setuptools/dist.py\", line 34, in <module>\n",
      "        from setuptools.config import parse_configuration\n",
      "      File \"/home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages/setuptools/config/__init__.py\", line 9, in <module>\n",
      "        from . import setupcfg\n",
      "      File \"/home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages/setuptools/config/setupcfg.py\", line 33, in <module>\n",
      "        from ..errors import FileError, OptionError\n",
      "    ImportError: cannot import name 'FileError' from 'setuptools.errors' (/home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages/setuptools/errors.py)\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/1a/43/e1d53588561e533212117750ee79ad0ba02a41f52a08c1df3396bd466c05/html2text-2024.2.26.tar.gz#sha256=05f8e367d15aaabc96415376776cdd11afd5127a77fce6e36afc60c563ca2c32 (from https://pypi.org/simple/html2text/) (requires-python:>=3.8). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /home/james/miniconda3/envs/n_llm_rag/bin/python -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-yxqeo1t_/html2text_04fb533f6636487c89296de1a0875574/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-yxqeo1t_/html2text_04fb533f6636487c89296de1a0875574/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-28q8fdna\n",
      "         cwd: /tmp/pip-install-yxqeo1t_/html2text_04fb533f6636487c89296de1a0875574/\n",
      "    Complete output (11 lines):\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages/setuptools/__init__.py\", line 18, in <module>\n",
      "        from setuptools.dist import Distribution\n",
      "      File \"/home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages/setuptools/dist.py\", line 34, in <module>\n",
      "        from setuptools.config import parse_configuration\n",
      "      File \"/home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages/setuptools/config/__init__.py\", line 9, in <module>\n",
      "        from . import setupcfg\n",
      "      File \"/home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages/setuptools/config/setupcfg.py\", line 33, in <module>\n",
      "        from ..errors import FileError, OptionError\n",
      "    ImportError: cannot import name 'FileError' from 'setuptools.errors' (/home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages/setuptools/errors.py)\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/78/5d/0ecf5c85c56d59fb14a123b3211957acd3e833f9c376a0a87ce3b0c8f433/html2text-2024.2.25.tar.gz#sha256=a197e7acaba5b5e46ff9f132b6fbeb89428c9155b5a971d7d9c343ebeb1019d3 (from https://pypi.org/simple/html2text/) (requires-python:>=3.8). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorforce 0.6.5 requires h5py~=3.1.0, but you have h5py 3.12.1 which is incompatible.\n",
      "tensorforce 0.6.5 requires msgpack>=1.0.2, but you have msgpack 0.6.2 which is incompatible.\n",
      "tensorforce 0.6.5 requires numpy==1.19.5, but you have numpy 1.26.4 which is incompatible.\n",
      "tensorforce 0.6.5 requires tensorflow==2.6.0, but you have tensorflow 2.18.0 which is incompatible.\n",
      "tensorflow-quantum 0.7.2 requires protobuf==3.17.3, but you have protobuf 5.28.3 which is incompatible.\n",
      "tensorflow-quantum 0.7.2 requires sympy==1.8, but you have sympy 1.13.1 which is incompatible.\n",
      "stable-baselines3 1.5.0 requires gym==0.21, but you have gym 0.26.2 which is incompatible.\n",
      "qcs-api-client 0.20.17 requires httpx<0.16.0,>=0.15.0, but you have httpx 0.27.2 which is incompatible.\n",
      "qcs-api-client 0.20.17 requires pydantic<2.0.0,>=1.7.2, but you have pydantic 2.9.2 which is incompatible.\n",
      "qcs-api-client 0.20.17 requires pyjwt<2.0.0,>=1.7.1, but you have pyjwt 2.8.0 which is incompatible.\n",
      "adversarial-robustness-toolbox 1.17.0 requires scikit-learn<1.2.0,>=0.22.2, but you have scikit-learn 1.4.0 which is incompatible.\u001b[0m\n",
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.3.5-py3-none-any.whl (2.4 MB)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from langchain-community) (0.1.139)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from langchain-community) (3.9.3)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0\n",
      "  Downloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from langchain-community) (9.0.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Collecting SQLAlchemy<2.0.36,>=1.4\n",
      "  Using cached SQLAlchemy-2.0.35-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from langchain-community) (0.3.15)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from langchain-community) (6.0)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.6 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from langchain-community) (0.3.7)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (20.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 3.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from langchain<0.4.0,>=0.3.6->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from langchain<0.4.0,>=0.3.6->langchain-community) (2.9.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-community) (1.33)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.11)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: idna in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (2.10)\n",
      "Requirement already satisfied: anyio in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (3.3.4)\n",
      "Requirement already satisfied: httpcore==1.* in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: certifi in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (2021.10.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain-community) (2.23.4)\n",
      "Collecting python-dotenv>=0.21.0\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (3.2.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages (from SQLAlchemy<2.0.36,>=1.4->langchain-community) (3.1.1)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: mypy-extensions, typing-inspect, SQLAlchemy, python-dotenv, marshmallow, pydantic-settings, httpx-sse, dataclasses-json, langchain-community\n",
      "  Attempting uninstall: SQLAlchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.36\n",
      "    Uninstalling SQLAlchemy-2.0.36:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.36\n",
      "Successfully installed SQLAlchemy-2.0.35 dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.5 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.6.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q -U torch torchvision datasets transformers==4.45.1 tensorflow langchain playwright html2text sentence_transformers faiss-cpu\n",
    "# !pip install -q accelerate peft==0.4.0 bitsandbytes==0.40.2 trl==0.4.7 # I did not install because my GPU only has 16 gb memory\n",
    "# !pip install -U langchain-community # Install the missing langchain-community package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3ffacc",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "1. go to https://huggingface.co/, create or login. At the top right icon, click settings -> access tokens -> create new token (click all permissions). Copy and paste it to the login line.\n",
    "\n",
    "2. if you get this error: `OSError: You are trying to access a gated repo.`, you need to go to https://huggingface.co/mistralai/Mistral-7B-v0.1 and accept usage terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4740c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "#     BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset\n",
    "# from peft import LoraConfig, PeftModel\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.document_loaders import AsyncChromiumLoader\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7c90c1",
   "metadata": {},
   "source": [
    "# Load (NOT quantized) Mistral7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2b3e712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d276b1e3cc7f4bfe8388efde623246dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Use your access token for read-only access\n",
    "login(token=\"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\")\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model_name='mistralai/Mistral-7B-Instruct-v0.1'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "# use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "# bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "# bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "# use_nested_quant = False\n",
    "\n",
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "# compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=use_4bit,\n",
    "#     bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "#     bnb_4bit_compute_dtype=compute_dtype,\n",
    "#     bnb_4bit_use_double_quant=use_nested_quant,\n",
    "# )\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "# if compute_dtype == torch.float16 and use_4bit:\n",
    "#     major, _ = torch.cuda.get_device_capability()\n",
    "#     if major >= 8:\n",
    "#         print(\"=\" * 80)\n",
    "#         print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "#         print(\"=\" * 80)\n",
    "\n",
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "#     quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2704271d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 7241732096\n",
      "all model parameters: 7241732096\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c81eed8",
   "metadata": {},
   "source": [
    "## Build Mistral text generation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2036350e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebebbf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f05e5eaf617f>:1: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n"
     ]
    }
   ],
   "source": [
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39f08e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context': '',\n",
       " 'question': 'who won the 2024 world series?',\n",
       " 'text': '\\n### [INST] Instruction: Answer the question based on your up to date sport knowledge. Here is context to help:\\n\\n\\n\\n### QUESTION:\\nwho won the 2024 world series? [/INST]\\n \\nThe 2024 World Series has not yet been played, so it is impossible to determine who will win it at this time. The World Series is an annual baseball championship that takes place in October and November, featuring the two teams with the best regular season records in Major League Baseball (MLB). The winner of the World Series is determined by a best-of-seven series between the American League champion and the National League champion.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"\"\"\n",
    "### [INST] Instruction: Answer the question based on your up to date sport knowledge. Here is context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question} [/INST]\n",
    " \"\"\"\n",
    "\n",
    "# Create prompt from prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# Create llm chain\n",
    "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)\n",
    "\n",
    "llm_chain.invoke({\"context\": \"\", \"question\": \"who won the 2024 world series?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b6fa61",
   "metadata": {},
   "source": [
    "## Load and chunk documents. Load chunked documents into FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4429ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install playwright\n",
    "# # !sudo playwright install\n",
    "# # !sudo playwright install-deps\n",
    "# !pip install html2text\n",
    "# !pip install sentence-transformers\n",
    "# !pip install faiss-cpu\n",
    "# # !pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "725f48af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Articles to index\n",
    "articles = [\"https://www.si.com/mlb/sports-illustrated-celebrates-dodgers-2024-world-series-commemorative-issue\",\n",
    "]\n",
    "\n",
    "# Scrapes the blogs above\n",
    "loader = AsyncChromiumLoader(articles)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fdb4e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3867bf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Converts HTML to plain text\n",
    "html2text = Html2TextTransformer()\n",
    "docs_transformed = html2text.transform_documents(docs)\n",
    "\n",
    "# Chunk text\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100,\n",
    "                                      chunk_overlap=0)\n",
    "chunked_documents = text_splitter.split_documents(docs_transformed)\n",
    "\n",
    "# Load chunked documents into the FAISS index\n",
    "db = FAISS.from_documents(chunked_documents, HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2', model_kwargs={'device':'cpu'}))\n",
    "\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebb4a183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/envs/n_llm_rag/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rag_chain = (\n",
    " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")\n",
    "\n",
    "result = rag_chain.invoke(\"Who won the 2024 world series?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b25c08cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### [INST] Instruction: Answer the question based on your up to date sport knowledge. Here is context to help:\n",
      "\n",
      "[Document(metadata={'source': 'https://www.si.com/mlb/sports-illustrated-celebrates-dodgers-2024-world-series-commemorative-issue'}, page_content='Error: Page.goto: Timeout 30000ms exceeded. Call log: navigating to\\n\"https://www.si.com/mlb/sports-illustrated-celebrates-dodgers-2024-world-\\nseries-commemorative-issue\", waiting until \"load\"')]\n",
      "\n",
      "### QUESTION:\n",
      "Who won the 2024 world series? [/INST]\n",
      " \n",
      "The Los Angeles Dodgers won the 2024 World Series, according to a Sports Illustrated commemorative issue celebrating their victory. However, it's important to note that this information is from a hypothetical scenario and the actual outcome of the 2024 World Series has not yet been determined.\n"
     ]
    }
   ],
   "source": [
    "result['context']\n",
    "print(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d949f119",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:n_llm_rag] *",
   "language": "python",
   "name": "conda-env-n_llm_rag-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
