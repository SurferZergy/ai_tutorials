{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb7b457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U bitsandbytes\n",
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/accelerate.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3ffacc",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "1. go to https://huggingface.co/, create or login. At the top right icon, click settings -> access tokens -> create new token (click all permissions). Copy and paste it to the login line.\n",
    "\n",
    "2. if you get this error: `OSError: You are trying to access a gated repo.`, you need to go to https://huggingface.co/mistralai/Mistral-7B-v0.1 and accept usage terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4754385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/james/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011583328247070312,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a8a2162fab42a19bd424349112f105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> def print_prime(n):\\n   \"\"\"\\n   Print all primes between 1 and n\\n   \"\"\"\\n   for i in range(2, n):\\n      if is_prime(i):\\n         print(i)\\n\\ndef is_prime(n):\\n   \"\"\"\\n   Check if n is prime\\n   \"\"\"\\n   for i in range(2, n):\\n      if n % i == 0:\\n         return False\\n   return True\\n\\nprint_prime(100)</s>'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "\n",
    "login(token=\"\") # your token\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ada41af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Prompt Result: Guess my favorate movie is the one that I have seen the most times.\n",
      "\n",
      "I have seen the movie \"The Sound of Music\" more than 100 times.\n",
      "\n",
      "I have seen it so many times that I can sing along with every song.\n",
      "\n",
      "I have seen it so many times that I can recite the entire script.\n",
      "\n",
      "I have seen it so many times that I can tell you what is going to happen next.\n",
      "\n",
      "I have seen it so many times that I can tell you what is going to happen after that.\n",
      "\n",
      "I have seen it so many times that I can tell you what is going to happen after that.\n",
      "\n",
      "I have seen it so many times that I can tell you what is going to happen after that.\n",
      "\n",
      "I have seen it so many times that I can tell you what is going to happen after that.\n",
      "\n",
      "I have seen it so many times that I can tell you what is going to happen after that.\n",
      "\n",
      "I have seen it so many times that I can tell you what is going to happen after that.\n",
      "\n",
      "I have seen it so many times that I can tell you what is going to happen after that.\n",
      "\n",
      "I have seen it so many times that I can tell\n"
     ]
    }
   ],
   "source": [
    "def promptf(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=30, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example simple prompt\n",
    "prompt_simple = \"Guess my favorate movie\"\n",
    "result_simple = promptf(prompt_simple)\n",
    "print(\"Simple Prompt Result:\", result_simple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f67cd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain of Thought Result: Guess my favorate movie. I enjoy documentaries. And I quite like to surf.\n",
      "\n",
      "I am a big fan of the movie \"The Endless Summer\". It is a documentary about two surfers who travel the world in search of the perfect wave. It is a classic and a must-see for any surfing enthusiast.\n",
      "\n",
      "I am a big fan of the movie \"The Endless Summer\". It is a documentary about two surfers who travel the world in search of the perfect wave. It is a classic and a must-see for any surfing enthusiast.\n",
      "\n",
      "I am a big fan of the movie \"The Endless Summer\". It is a documentary about two surfers who travel the world in search of the perfect wave. It is a classic and a must-see for any surfing enthusiast.\n",
      "\n",
      "I am a big fan of the movie \"The Endless Summer\". It is a documentary about two surfers who travel the world in search of the perfect wave. It is a classic and a must-see for any surfing enthusiast.\n",
      "\n",
      "I am a big fan of the movie \"The Endless Summer\". It is a documentary about two surfers who travel the world in search of the perfect wave. It is a classic and a must-see for any surfing enthusiast.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_cot = \"Guess my favorate movie. I enjoy documentaries. And I quite like to surf.\"\n",
    "result_cot = promptf(prompt_cot)\n",
    "print(\"Chain of Thought Result:\", result_cot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4851f9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
