{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8b394aa",
   "metadata": {},
   "source": [
    "## We will fine tune using state of the art method qlora. QLoRA (Quantized Low-Rank Adapters) is a fine-tuning method designed to make the process of fine-tuning large language models more efficient. It combines two techniques: quantization and low-rank adapters (LoRA).\n",
    "\n",
    "Quantization: Reduces the numerical precision of model weights, making the model more compact and faster to execute. This involves mapping high-precision floating-point values to lower-precision values, such as 4-bit integers1.\n",
    "\n",
    "Low-Rank Adapters (LoRA): Fine-tunes large pre-trained models by updating only a small number of trainable parameters, reducing computational complexity and memory usage.\n",
    "\n",
    "Efficiency: Allows fine-tuning of massive models with billions of parameters on relatively small GPUs.\n",
    "\n",
    "Cost-Effective: Reduces the need for expensive, high-memory computing resources.\n",
    "\n",
    "Performance: Maintains high performance while being more memory-efficient.\n",
    "\n",
    "QLoRA democratizes fine-tuning by making it accessible to those with limited resources, enabling state-of-the-art results even with smaller models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e2190a",
   "metadata": {},
   "source": [
    "##### Install libraries, only need to do this once per instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e70fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes\n",
    "!pip install -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -U datasets scipy ipywidgets wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e148611-2759-41a6-b2c2-89cf88476f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: accelerate\n",
      "Version: 1.2.0.dev0\n",
      "Summary: Accelerate\n",
      "Home-page: https://github.com/huggingface/accelerate\n",
      "Author: The HuggingFace team\n",
      "Author-email: zach.mueller@huggingface.co\n",
      "License: Apache\n",
      "Location: /home/calee/.local/lib/python3.11/site-packages\n",
      "Requires: huggingface_hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
      "Required-by: peft\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3ffacc",
   "metadata": {},
   "source": [
    "##### NOTE:\n",
    "1. go to https://huggingface.co/, create or login. At the top right icon, click settings -> access tokens -> create new token (click all permissions). Copy and paste it to the login line.  (No login line)\n",
    "\n",
    "2. if you get this error: `OSError: You are trying to access a gated repo.`, you need to go to https://huggingface.co/mistralai/Mistral-7B-v0.1 and accept usage terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4740c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7c90c1",
   "metadata": {},
   "source": [
    "### gem/viggo is video lingo training data\n",
    "\n",
    "we see that it contains 5k training samples\n",
    "Note we do not need to login to HF to use this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d84d16a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['gem_id', 'meaning_representation', 'target', 'references'],\n",
      "    num_rows: 5103\n",
      "})\n",
      "Dataset({\n",
      "    features: ['gem_id', 'meaning_representation', 'target', 'references'],\n",
      "    num_rows: 714\n",
      "})\n",
      "Dataset({\n",
      "    features: ['gem_id', 'meaning_representation', 'target', 'references'],\n",
      "    num_rows: 1083\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('gem/viggo', split='train')\n",
    "eval_dataset = load_dataset('gem/viggo', split='validation')\n",
    "test_dataset = load_dataset('gem/viggo', split='test')\n",
    "\n",
    "print(train_dataset)\n",
    "print(eval_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682e79cb",
   "metadata": {},
   "source": [
    "##### But we still need to login to use the mistral LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cac8930e-90de-4db4-8d32-40f7925e8939",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: huggingface_hub in /home/calee/.local/lib/python3.11/site-packages (0.26.2)\n",
      "Requirement already satisfied: filelock in /home/calee/.local/lib/python3.11/site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/calee/.local/lib/python3.11/site-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /mnt/opt/miniconda3/lib/python3.11/site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/opt/miniconda3/lib/python3.11/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /mnt/opt/miniconda3/lib/python3.11/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /mnt/opt/miniconda3/lib/python3.11/site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/opt/miniconda3/lib/python3.11/site-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/opt/miniconda3/lib/python3.11/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/opt/miniconda3/lib/python3.11/site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/opt/miniconda3/lib/python3.11/site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/opt/miniconda3/lib/python3.11/site-packages (from requests->huggingface_hub) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14fa5b91-bc87-4824-80c9-1b01d578e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=\"\") # your token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b53fa3",
   "metadata": {},
   "source": [
    "##### Load LLM with bnb for quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0232aa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5555ef86e0a346aca5b2737e76c04468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8d68b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    model_max_length=512,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "056d06a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16b562b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt =f\"\"\"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.\n",
    "This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].\n",
    "The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\n",
    "\n",
    "\n",
    "### Target sentence:\n",
    "{data_point[\"target\"]}\n",
    "\n",
    "\n",
    "### Meaning representation:\n",
    "{data_point[\"meaning_representation\"]}\n",
    "\"\"\"\n",
    "    return tokenize(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a16acc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 12628, 264, 2718, 12271, 5122, 272, 14164, 5746, 9283, 302, 272, 2787, 12271, 390, 264, 2692, 908, 395, 9623, 304, 6836, 3069, 28723, 13, 3260, 908, 1023, 6685, 272, 2718, 1423, 24329, 304, 272, 908, 1580, 347, 624, 302, 272, 2296, 5936, 262, 674, 647, 464, 3134, 647, 464, 28721, 495, 28730, 410, 262, 296, 647, 464, 19928, 647, 464, 14876, 28730, 9122, 647, 464, 28713, 16939, 647, 464, 3134, 28730, 720, 11009, 352, 647, 464, 267, 1805, 416, 647, 464, 3134, 28730, 9122, 14303, 13, 1014, 9623, 1580, 347, 624, 302, 272, 2296, 28747, 5936, 861, 647, 464, 5128, 28730, 11023, 28730, 1408, 647, 464, 11023, 28730, 4395, 647, 464, 16239, 263, 647, 464, 274, 9312, 647, 464, 28599, 647, 464, 2383, 411, 647, 464, 7449, 28730, 4837, 8524, 647, 464, 3537, 28730, 13102, 7449, 647, 464, 10470, 28713, 647, 464, 13952, 28730, 266, 28730, 2453, 314, 647, 464, 3537, 28730, 8502, 28730, 11023, 647, 464, 3537, 28730, 7502, 28730, 11023, 647, 464, 4101, 3591, 1421, 13, 13, 13, 27332, 15255, 12271, 28747, 13, 3195, 28742, 28713, 272, 1080, 9783, 2039, 369, 368, 4226, 297, 272, 879, 28705, 28750, 28734, 28740, 28781, 28804, 13, 13, 13, 27332, 11736, 288, 9283, 28747, 13, 3134, 28732, 11023, 28730, 4395, 28792, 28750, 28734, 28740, 28781, 1181, 1229, 3591, 28792, 360, 7030, 2803, 13, 2]\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)\n",
    "print(tokenized_train_dataset[4]['input_ids'])\n",
    "print(len(tokenized_train_dataset[4]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6adb1",
   "metadata": {},
   "source": [
    "##### above we see the tokens representing the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3be9d7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Sentence: Earlier, you stated that you didn't have strong feelings about PlayStation's Little Big Adventure. Is your opinion true for all games which don't have multiplayer?\n",
      "Meaning Representation: verify_attribute(name[Little Big Adventure], rating[average], has_multiplayer[no], platforms[PlayStation])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Target Sentence: \" + test_dataset[1]['target'])\n",
    "print(\"Meaning Representation: \" + test_dataset[1]['meaning_representation'] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f18083",
   "metadata": {},
   "source": [
    "### Test the model before fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13fd7ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.\n",
      "This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].\n",
      "The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\n",
      "\n",
      "\n",
      "### Target sentence:\n",
      "Earlier, you stated that you didn't have strong feelings about PlayStation's Little Big Adventure. Is your opinion true for all games which don't have multiplayer?\n",
      "\n",
      "\n",
      "### Meaning representation:\n",
      "Ð´Ð»Ñ_all_games_which_don't_have_multiplayer_is_your_opinion_true_for_playstation's_little_big_adventure\n",
      "\n",
      "\n",
      "### Meaning representation:\n",
      "Is_your_opinion_true_for_playstation's_little_big_adventure_for_all_games_which_don't_have_multiplayer\n",
      "\n",
      "\n",
      "### Meaning representation:\n",
      "Is_your_opinion_true_for_all_games_which_don't_have_multiplayer_for_playstation's_little_big_adventure\n",
      "\n",
      "\n",
      "### Meaning representation:\n",
      "Is_your_opinion_true_for_playstation's_little_big_adventure_for_all_games_which_don't_have_multiplayer\n",
      "\n",
      "\n",
      "### Meaning representation:\n",
      "Is_your_opinion_true_for_all_games_which_don't_have_multiplayer_for_playstation's_little_big_adventure\n",
      "\n",
      "\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "# from peft import PeftModel\n",
    "# offload_dir = \"./offload_dir\"\n",
    "\n",
    "# model = PeftModel.from_pretrained(model, base_model_id, offload_folder=offload_dir)\n",
    "\n",
    "eval_prompt = \"\"\"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.\n",
    "This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].\n",
    "The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\n",
    "\n",
    "\n",
    "### Target sentence:\n",
    "Earlier, you stated that you didn't have strong feelings about PlayStation's Little Big Adventure. Is your opinion true for all games which don't have multiplayer?\n",
    "\n",
    "\n",
    "### Meaning representation:\n",
    "\"\"\"\n",
    "\n",
    "# import torch\n",
    "\n",
    "# Define the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the same device\n",
    "model = model.to(device)\n",
    "\n",
    "# Tokenize the input\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(device)  # Move input to the same device\n",
    "\n",
    "# Set the model to evaluation mode and generate output without gradient tracking\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**model_input, max_new_tokens=256, pad_token_id=2)\n",
    "    \n",
    "    # Decode the generated ids and print the result\n",
    "    print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "# model_input = tokenizer(eval_prompt, return_tensors=\"pt\")\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaa30fb",
   "metadata": {},
   "source": [
    "##### above we see the LLM trying to understand the meaning, but the output is alittle weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e351210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa2c797",
   "metadata": {},
   "source": [
    "##### below defines the LoRA config then prints the number of trainable parameters (ones that requires gradient)\n",
    "\n",
    "r=8: This parameter controls the rank of the LoRA, determining the number of components used to approximate the original weight matrices.\n",
    "\n",
    "lora_alpha=16: This is a scaling factor for the LoRA updates.\n",
    "\n",
    "target_modules: A list of specific model components to apply LoRA to, such as \"q_proj\", \"k_proj\", and so on. \n",
    "\n",
    "- In the self-attention mechanism, the input is projected into three different spaces: queries (q_proj), keys (k_proj), and values (v_proj). Here's a brief explanation of these projections:\n",
    "    - Queries (q_proj): These are the transformed representations of the input that are used to match against keys.\n",
    "\n",
    "    - Keys (k_proj): These represent the input data and are matched against the queries to determine relevance.\n",
    "\n",
    "    - Values (v_proj): These carry the actual information and are combined based on the attention scores calculated from the queries and keys.\n",
    "    \n",
    "- o_proj (Output Projection):\n",
    "\n",
    "    This layer projects the output of the attention mechanism into the same dimension as the input. It's responsible for transforming the attention output before it is passed to the next layer in the network.\n",
    "\n",
    "- gate_proj (Gating Projection):\n",
    "\n",
    "    This layer is used in gated mechanisms, such as in Gated Recurrent Units (GRUs) or certain transformer variants. It controls the flow of information through the network by gating signals, often determining which parts of the input should be emphasized or suppressed.\n",
    "\n",
    "- up_proj (Up Projection) and down_proj (Down Projection):\n",
    "\n",
    "    These terms are typically used in models that involve dimensionality changes.\n",
    "\n",
    "    up_proj: This layer increases the dimensionality of the data. It's often used in layers where more expressive power is needed, like when transitioning from a compressed representation to a higher-dimensional space.\n",
    "\n",
    "    down_proj: This layer decreases the dimensionality of the data. It's useful for reducing the computational load by compressing representations, often after processing or combining features.\n",
    "\n",
    "- lm_head (Language Model Head):\n",
    "\n",
    "    This is the final layer in a language model used to produce the output. It converts the hidden states of the model into logits, which represent the probabilities of the next token in sequence modeling tasks.\n",
    "\n",
    "    These projections allow the model to weigh different parts of the input differently, enabling it to focus on the most relevant information for a particular task.\n",
    "\n",
    "bias=\"none\": This indicates whether to use bias terms in the LoRA layers.\n",
    "\n",
    "lora_dropout=0.05: This specifies the dropout rate applied to the LoRA layers.\n",
    "\n",
    "task_type=\"CAUSAL_LM\": This defines the task type, in this case, causal language modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8280394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 21260288 || all params: 3773331456 || trainable%: 0.5634354746703705\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "# model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d949f119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): MistralRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "      (lm_head): lora.Linear(\n",
      "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.05, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=8, out_features=32000, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "        (lora_magnitude_vector): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25234585",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaea14c",
   "metadata": {},
   "source": [
    "### Perform fine tuning training\n",
    "\n",
    "model: The model to be trained.\n",
    "\n",
    "train_dataset and eval_dataset: The datasets for training and evaluation.\n",
    "\n",
    "args: The training arguments, including:\n",
    "\n",
    "output_dir: Directory to save the model.\n",
    "\n",
    "warmup_steps: Number of warm-up steps.\n",
    "\n",
    "per_device_train_batch_size: Batch size per device.\n",
    "\n",
    "gradient_accumulation_steps: Number of steps to accumulate gradients before updating.\n",
    "\n",
    "max_steps: Maximum number of training steps.\n",
    "\n",
    "learning_rate: Learning rate for optimization.\n",
    "\n",
    "logging_steps: Frequency of logging updates.\n",
    "\n",
    "bf16: Whether to use bfloat16 precision.\n",
    "\n",
    "optim: Optimizer to use (paged_adamw_8bit or adamw_hf).\n",
    "\n",
    "logging_dir: Directory for logs.\n",
    "\n",
    "save_strategy and save_steps: Strategy and frequency of saving checkpoints.\n",
    "\n",
    "evaluation_strategy and eval_steps: Strategy and frequency of evaluation.\n",
    "\n",
    "do_eval: Whether to perform evaluation.\n",
    "\n",
    "report_to: Reporting mechanism (e.g., \"wandb\" for Weights & Biases).\n",
    "\n",
    "data_collator: The data collator for language modeling, specifying not to use masked language modeling (mlm=False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d9afc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/calee/.local/lib/python3.11/site-packages/transformers/training_args.py:1570: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/home/calee/.local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  82/1000 37:45 < 7:13:19, 0.04 it/s, Epoch 0.13/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.742200</td>\n",
       "      <td>0.271674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/calee/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/calee/.local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "project = \"viggo-finetune\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=5,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_steps=1000,\n",
    "        learning_rate=2.5e-5, # Want about 10x smaller than the Mistral learning rate\n",
    "        logging_steps=50,\n",
    "        bf16=False,\n",
    "        optim=\"paged_adamw_8bit\", \n",
    "#         optim=\"adamw_hf\",\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=50,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        report_to=\"none\",           # Use \"wandb\" if you want to use wandb\n",
    "#         run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\",          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25b0a1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/calee/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785ca9d0abe047d7a304796412e8f084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dee7cb3",
   "metadata": {},
   "source": [
    "## Test the model after fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e89a6198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.\n",
      "This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].\n",
      "The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\n",
      "\n",
      "\n",
      "### Target sentence:\n",
      "Earlier, you stated that you didn't have strong feelings about PlayStation's Little Big Adventure. Is your opinion true for all games which don't have multiplayer?\n",
      "\n",
      "\n",
      "### Meaning representation:\n",
      " Given a game without multiplayer, do you feel indifferent about it?\n",
      "\n",
      "\n",
      "### Target sentence:\n",
      "I'm curious, why do you think that the PlayStation is a good platform for games?\n",
      "\n",
      "\n",
      "### Meaning representation:\n",
      "request_explanation(rating[good], platforms[PlayStation])\n",
      "\n",
      "\n",
      "### Target sentence:\n",
      "I'm curious, why do you think that the PlayStation is a poor platform for games?\n",
      "\n",
      "\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from peft import PeftModel\n",
    "# ft_model = PeftModel.from_pretrained(base_model, \"mistral-viggo-finetune/checkpoint-1000\")\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"mistral-viggo-finetune/checkpoint-1000-full\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, pad_token_id=2)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1246716f",
   "metadata": {},
   "source": [
    "##### After fine tuning, we see the LLM understands the meaning of the sentence better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a26ee7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:n_llm_ft]",
   "language": "python",
   "name": "conda-env-n_llm_ft-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
