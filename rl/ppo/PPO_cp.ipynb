{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253e8dbf",
   "metadata": {},
   "source": [
    "Steps:\n",
    "- import libraries\n",
    "- setup hyperparameters\n",
    "- define a policy (actor) network. Notice how this network is relatively small. Input dims is the shape of the state space. Output dim is the shape of the action space.\n",
    "- define a value (critic) network.\n",
    "- PPO is an on-policy algorithm, meaning it learns/updates the policy based on actions taken by the same policy. \n",
    "    - one might ask, why do you need a memory store for experience for an on policy algorithm? We are using it to:\n",
    "        - perform batch updates\n",
    "        - calculate (monte carlo) returns and advantages using GAE\n",
    "- Run training\n",
    "\n",
    "Next steps:\n",
    "- Run inference\n",
    "- Save video of one episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5cf6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLength: 15\n",
      "Episode 500\tLength: 26\n",
      "Episode 1000\tLength: 22\n",
      "Episode 1500\tLength: 37\n",
      "Episode 2000\tLength: 72\n",
      "Episode 2500\tLength: 377\n",
      "Episode 3000\tLength: 9999\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 3e-4\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "eps_clip = 0.2\n",
    "K_epochs = 4\n",
    "update_timestep = 2000\n",
    "\n",
    "# Create the CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Define the policy network (actor)\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        action_probs = torch.softmax(self.fc3(x), dim=-1)\n",
    "        return action_probs\n",
    "\n",
    "# Define the value network (critic)\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        value = self.fc3(x)\n",
    "        return value\n",
    "\n",
    "# PPO Agent\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
    "        self.policy_old = PolicyNetwork(state_dim, action_dim)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.value_function = ValueNetwork(state_dim)\n",
    "        self.vf_optimizer = optim.Adam(self.value_function.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "        self.action_dim = action_dim\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state)\n",
    "        action_probs = self.policy_old(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "    \n",
    "    def update(self, memory):\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + gamma * discounted_reward\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        \n",
    "        # Normalize rewards\n",
    "        rewards = torch.tensor(rewards)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "        \n",
    "        # Convert list to tensor\n",
    "        old_states = torch.FloatTensor(memory.states)\n",
    "        old_actions = torch.LongTensor(memory.actions)\n",
    "        old_logprobs = torch.FloatTensor(memory.logprobs)\n",
    "\n",
    "        \n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(K_epochs):\n",
    "            # Compute the advantage\n",
    "            values = self.value_function(old_states).squeeze()\n",
    "            advantages = rewards - values.detach()\n",
    "\n",
    "            # Get action probabilities from the current policy\n",
    "            action_probs = self.policy(old_states)\n",
    "            dist = Categorical(action_probs)\n",
    "            new_logprobs = dist.log_prob(old_actions)\n",
    "            dist_entropy = dist.entropy()\n",
    "\n",
    "            # Importance sampling ratio\n",
    "            ratios = torch.exp(new_logprobs - old_logprobs)\n",
    "\n",
    "            # Compute surrogate loss\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - eps_clip, 1 + eps_clip) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            # Optimize policy\n",
    "            self.optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Value loss\n",
    "            value_loss = self.MseLoss(values, rewards)\n",
    "\n",
    "            # Optimize value function\n",
    "            self.vf_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            self.vf_optimizer.step()\n",
    "        \n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "# Memory to store experiences\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        del self.states[:]\n",
    "        del self.actions[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "# Main training loop\n",
    "def train_ppo():\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    ppo = PPO(state_dim, action_dim)\n",
    "    memory = Memory()\n",
    "\n",
    "    timestep = 0\n",
    "    for i_episode in range(10000):\n",
    "        state, _ = env.reset()\n",
    "        for t in range(1, 10000):\n",
    "            timestep += 1\n",
    "\n",
    "            # Select action using policy\n",
    "            action, logprob = ppo.select_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            \n",
    "            # Store in memory\n",
    "            memory.states.append(state)\n",
    "            memory.actions.append(action)\n",
    "            memory.logprobs.append(logprob)\n",
    "            memory.rewards.append(reward)\n",
    "            memory.is_terminals.append(done)\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "\n",
    "            # If episode is done, break the loop\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # Update PPO every update_timestep\n",
    "            if timestep % update_timestep == 0:\n",
    "                ppo.update(memory)\n",
    "                memory.clear_memory()\n",
    "                timestep = 0\n",
    "\n",
    "        # Print episode results\n",
    "        if i_episode % 500 == 0:\n",
    "            print(f'Episode {i_episode}\\tLength: {t}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_ppo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6da9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
