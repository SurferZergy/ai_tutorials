{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2370610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install popgym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc5bfb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from popgym.envs.minesweeper import MineSweeperEasy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39e94f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " . . . .\n",
      " . . . .\n",
      " . . . .\n",
      " . . . .\n",
      "input index:1,1\n",
      "<class 'int'>\n",
      "reward: 0.07142857142857142\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-28d4ec13673e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input index:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0maction_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_rllib/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m             )\n\u001b[0;32m--> 848\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    849\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_rllib/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "game = MineSweeperEasy()\n",
    "done = False\n",
    "obs, info = game.reset()\n",
    "reward = -float(\"inf\")\n",
    "game.render()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = input(\"input index:\").split(\",\")\n",
    "    action_int = (int(action[0]), int(action[1]))\n",
    "    obs, reward, truncated, terminated, info = game.step(action_int)\n",
    "    done = truncated or terminated\n",
    "    print(type(obs))\n",
    "    # game.render()\n",
    "    print(\"reward:\", reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb9578a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " . . . .\n",
      " . . . .\n",
      " . . . .\n",
      " . . . .\n",
      "o 0 i {}\n",
      "0\n",
      " . . 0 .\n",
      " . . . .\n",
      " . . . .\n",
      " . . . .\n",
      "reward: 0.07142857142857142\n",
      "1\n",
      " . . 0 .\n",
      " . . . .\n",
      " . . . 1\n",
      " . . . .\n",
      "reward: 0.07142857142857142\n",
      "0\n",
      " 0 . 0 .\n",
      " . . . .\n",
      " . . . 1\n",
      " . . . .\n",
      "reward: 0.07142857142857142\n",
      "1\n",
      " 0 . 0 .\n",
      " 1 . . .\n",
      " . . . 1\n",
      " . . . .\n",
      "reward: 0.07142857142857142\n",
      "1\n",
      " 0 . 0 .\n",
      " 1 . . 1\n",
      " . . . 1\n",
      " . . . .\n",
      "reward: 0.07142857142857142\n",
      "1\n",
      " 0 . 0 .\n",
      " 1 . . 1\n",
      " . . . 1\n",
      " . . . .\n",
      "reward: -0.041666666666666664\n",
      "1\n",
      " 0 . 0 .\n",
      " 1 . . 1\n",
      " . . . 1\n",
      " . . . 1\n",
      "reward: 0.07142857142857142\n",
      "1\n",
      " 0 . 0 .\n",
      " 1 . 1 1\n",
      " . . . 1\n",
      " . . . 1\n",
      "reward: 0.07142857142857142\n",
      "0\n",
      " 0 . 0 0\n",
      " 1 . 1 1\n",
      " . . . 1\n",
      " . . . 1\n",
      "reward: 0.07142857142857142\n",
      "0\n",
      " 0 . 0 0\n",
      " 1 . 1 1\n",
      " . . . 1\n",
      " . . . 1\n",
      "reward: -0.041666666666666664\n",
      "1\n",
      " 0 . 0 0\n",
      " 1 . 1 1\n",
      " . . . 1\n",
      " . . . 1\n",
      "reward: -0.041666666666666664\n",
      "1\n",
      " 0 . 0 0\n",
      " 1 . 1 1\n",
      " . . . 1\n",
      " . . . 1\n",
      "reward: -0.041666666666666664\n",
      "1\n",
      " 0 . 0 0\n",
      " 1 . 1 1\n",
      " . . . 1\n",
      " . . . 1\n",
      "reward: -0.041666666666666664\n",
      "1\n",
      " 0 . 0 0\n",
      " 1 . 1 1\n",
      " . . . 1\n",
      " . . . 1\n",
      "reward: -0.041666666666666664\n",
      "1\n",
      " 0 . 0 0\n",
      " 1 . 1 1\n",
      " . . . 1\n",
      " . . . 1\n",
      "reward: -0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize the game\n",
    "game = MineSweeperEasy()\n",
    "done = False\n",
    "obs, info = game.reset()\n",
    "reward = -float(\"inf\")\n",
    "game.render()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # Select a random action\n",
    "    action_int = (np.random.randint(0, 4), np.random.randint(0, 4))\n",
    "    \n",
    "    # Take a step in the game\n",
    "    obs, reward, truncated, terminated, info = game.step(action_int)\n",
    "    done = truncated or terminated\n",
    "    \n",
    "    # Display the type of observation and the reward\n",
    "    print(obs)\n",
    "    game.render()  # Uncomment to see the game render\n",
    "    print(\"reward:\", reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae2db6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Discrete(3) observation space is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9cb06a4c5936>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Initialize the PPO model with Stable-Baselines3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MlpPolicy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Instantiate the custom callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_rllib/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, target_kl, tensorboard_log, create_eval_env, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_init_setup_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_rllib/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36m_setup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPPO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m# Initialize schedules for policy/value clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_rllib/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36m_setup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mbuffer_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictRolloutBuffer\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mRolloutBuffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         self.rollout_buffer = buffer_cls(\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_rllib/lib/python3.9/site-packages/stable_baselines3/common/buffers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, buffer_size, observation_space, action_space, device, gae_lambda, gamma, n_envs)\u001b[0m\n\u001b[1;32m    340\u001b[0m     ):\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRolloutBuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_envs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgae_lambda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgae_lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_rllib/lib/python3.9/site-packages/stable_baselines3/common/buffers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, buffer_size, observation_space, action_space, device, n_envs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_obs_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_action_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/n_rllib/lib/python3.9/site-packages/stable_baselines3/common/preprocessing.py\u001b[0m in \u001b[0;36mget_obs_shape\u001b[0;34m(observation_space)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{observation_space} observation space is not supported\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Discrete(3) observation space is not supported"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Custom wrapper to flatten MultiDiscrete action space\n",
    "class FlattenedActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(FlattenedActionWrapper, self).__init__(env)\n",
    "        # Flatten MultiDiscrete([4, 4]) to Discrete(16)\n",
    "        self.action_space = gym.spaces.Discrete(env.action_space.nvec[0] * env.action_space.nvec[1])\n",
    "\n",
    "    def action(self, action):\n",
    "        # Map discrete action back to 2D grid coordinates\n",
    "        return (action // self.env.action_space.nvec[1], action % self.env.action_space.nvec[1])\n",
    "\n",
    "# Custom callback to print mean reward\n",
    "class RewardPrintingCallback(BaseCallback):\n",
    "    def __init__(self, eval_env, eval_freq=1000, n_eval_episodes=10, verbose=1):\n",
    "        super(RewardPrintingCallback, self).__init__(verbose)\n",
    "        self.eval_env = eval_env\n",
    "        self.eval_freq = eval_freq\n",
    "        self.n_eval_episodes = n_eval_episodes\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.eval_freq == 0:\n",
    "            rewards = []\n",
    "            for _ in range(self.n_eval_episodes):\n",
    "                obs = self.eval_env.reset()\n",
    "                done = False\n",
    "                total_reward = 0\n",
    "                while not done:\n",
    "                    action, _states = self.model.predict(obs, deterministic=True)\n",
    "                    obs, reward, done, info = self.eval_env.step(action)\n",
    "                    total_reward += reward\n",
    "                rewards.append(total_reward)\n",
    "            mean_reward = np.mean(rewards)\n",
    "            print(f\"Step {self.n_calls}: Mean Reward over {self.n_eval_episodes} episodes: {mean_reward}\")\n",
    "        return True\n",
    "\n",
    "# Create and wrap the Minesweeper environment\n",
    "env = MineSweeperEasy()\n",
    "env = FlattenedActionWrapper(env)\n",
    "\n",
    "# Initialize the PPO model with Stable-Baselines3\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Instantiate the custom callback\n",
    "reward_callback = RewardPrintingCallback(eval_env=env, eval_freq=1000, n_eval_episodes=10)\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=10000, callback=reward_callback)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"ppo_minesweeper_easy\")\n",
    "\n",
    "# Optionally, load the model and evaluate\n",
    "model = PPO.load(\"ppo_minesweeper_easy\")\n",
    "obs = env.reset()\n",
    "\n",
    "# Run an episode and display the results\n",
    "for _ in range(100):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9c4b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:n_rllib] *",
   "language": "python",
   "name": "conda-env-n_rllib-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
