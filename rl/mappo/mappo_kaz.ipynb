{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb9044f1",
   "metadata": {},
   "source": [
    "The KAZ environment features 4 cooperative agents: 2 archers and 2 knights. Each time step, only one agent takes an action, which is different from many other MARL environments where all agents act simultaneously (joint action).\n",
    "\n",
    "In MAPPO, each agent has its own policy (actor), but all 4 agents share a centralized critic value network.\n",
    "\n",
    "Similar to single-agent PPO, the policy optimizes Generalized Advantage Estimation (GAE) and uses the PPO clipping method to prevent overly large updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9d4888a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-30c9766906f4>:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  advantages = torch.tensor(trajectories[agent][\"advantages\"])\n",
      "<ipython-input-64-30c9766906f4>:189: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  returns = torch.tensor(trajectories[agent][\"returns\"])\n",
      "<ipython-input-64-30c9766906f4>:224: UserWarning: Using a target size (torch.Size([972])) that is different to the input size (torch.Size([243, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = F.mse_loss(self.value_network(all_obs), all_returns)\n",
      "<ipython-input-64-30c9766906f4>:231: UserWarning: Using a target size (torch.Size([972])) that is different to the input size (torch.Size([243, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = F.mse_loss(self.value_network(all_obs), all_returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Loss: -0.2715325653553009\n",
      "Policy Loss: -0.0636511743068695\n",
      "Policy Loss: -0.13452228903770447\n",
      "Policy Loss: 0.0041281115263700485\n",
      "Value Loss: 0.07578323036432266\n",
      "Policy Loss: -0.27215665578842163\n",
      "Policy Loss: -0.06395678222179413\n",
      "Policy Loss: -0.13486842811107635\n",
      "Policy Loss: 0.004096361342817545\n",
      "Value Loss: 0.06358543038368225\n",
      "Policy Loss: -0.27271559834480286\n",
      "Policy Loss: -0.0642416700720787\n",
      "Policy Loss: -0.1351667195558548\n",
      "Policy Loss: 0.004071083385497332\n",
      "Value Loss: 0.0639866292476654\n",
      "Policy Loss: -0.2732594907283783\n",
      "Policy Loss: -0.06450287252664566\n",
      "Policy Loss: -0.13543599843978882\n",
      "Policy Loss: 0.004052154719829559\n",
      "Value Loss: 0.06528209894895554\n",
      "Policy Loss: -0.2737853229045868\n",
      "Policy Loss: -0.0647490993142128\n",
      "Policy Loss: -0.13570371270179749\n",
      "Policy Loss: 0.004037959035485983\n",
      "Value Loss: 0.06403915584087372\n",
      "Policy Loss: -0.2742953300476074\n",
      "Policy Loss: -0.06498785316944122\n",
      "Policy Loss: -0.13594244420528412\n",
      "Policy Loss: 0.00402627931907773\n",
      "Value Loss: 0.06265632063150406\n",
      "Policy Loss: -0.2747845947742462\n",
      "Policy Loss: -0.06522228568792343\n",
      "Policy Loss: -0.1361539363861084\n",
      "Policy Loss: 0.0040161567740142345\n",
      "Value Loss: 0.06213913857936859\n",
      "Policy Loss: -0.27527767419815063\n",
      "Policy Loss: -0.06545300036668777\n",
      "Policy Loss: -0.13635824620723724\n",
      "Policy Loss: 0.004007038660347462\n",
      "Value Loss: 0.062233854085206985\n",
      "Policy Loss: -0.27580153942108154\n",
      "Policy Loss: -0.06565187871456146\n",
      "Policy Loss: -0.13657309114933014\n",
      "Policy Loss: 0.003998422995209694\n",
      "Value Loss: 0.06248094141483307\n",
      "Policy Loss: -0.2763272821903229\n",
      "Policy Loss: -0.06582249701023102\n",
      "Policy Loss: -0.13680492341518402\n",
      "Policy Loss: 0.003990035504102707\n",
      "Value Loss: 0.06262120604515076\n",
      "Episode 1 | Reward: 7 | Policy Loss: [-0.2763272821903229, -0.06582249701023102, -0.13680492341518402, 0.003990035504102707] | Value Loss: [0.06262120604515076]\n",
      "Policy Loss: -0.17750099301338196\n",
      "Policy Loss: 0.02608419582247734\n",
      "Policy Loss: 0.025973862037062645\n",
      "Policy Loss: 0.02588413655757904\n",
      "Value Loss: 0.025597646832466125\n",
      "Policy Loss: -0.17758730053901672\n",
      "Policy Loss: 0.026109308004379272\n",
      "Policy Loss: 0.025991152971982956\n",
      "Policy Loss: 0.02581717073917389\n",
      "Value Loss: 0.0253717303276062\n",
      "Policy Loss: -0.17773766815662384\n",
      "Policy Loss: 0.02610555663704872\n",
      "Policy Loss: 0.025984780862927437\n",
      "Policy Loss: 0.025731850415468216\n",
      "Value Loss: 0.025133734568953514\n",
      "Policy Loss: -0.17793631553649902\n",
      "Policy Loss: 0.02608051337301731\n",
      "Policy Loss: 0.025958864018321037\n",
      "Policy Loss: 0.02563367411494255\n",
      "Value Loss: 0.024994825944304466\n",
      "Policy Loss: -0.17818237841129303\n",
      "Policy Loss: 0.02603997476398945\n",
      "Policy Loss: 0.025917332619428635\n",
      "Policy Loss: 0.02552352286875248\n",
      "Value Loss: 0.024994956329464912\n",
      "Policy Loss: -0.17845754325389862\n",
      "Policy Loss: 0.025988463312387466\n",
      "Policy Loss: 0.02586117386817932\n",
      "Policy Loss: 0.025403201580047607\n",
      "Value Loss: 0.02508535422384739\n",
      "Policy Loss: -0.17876079678535461\n",
      "Policy Loss: 0.025927133858203888\n",
      "Policy Loss: 0.025794927030801773\n",
      "Policy Loss: 0.025272158905863762\n",
      "Value Loss: 0.02516196109354496\n",
      "Policy Loss: -0.17907942831516266\n",
      "Policy Loss: 0.025858424603939056\n",
      "Policy Loss: 0.02571890316903591\n",
      "Policy Loss: 0.025134628638625145\n",
      "Value Loss: 0.025161689147353172\n",
      "Policy Loss: -0.17940214276313782\n",
      "Policy Loss: 0.025785326957702637\n",
      "Policy Loss: 0.025638269260525703\n",
      "Policy Loss: 0.02500145696103573\n",
      "Value Loss: 0.025102056562900543\n",
      "Policy Loss: -0.17971687018871307\n",
      "Policy Loss: 0.02571047469973564\n",
      "Policy Loss: 0.025555619969964027\n",
      "Policy Loss: 0.02488286793231964\n",
      "Value Loss: 0.02502855658531189\n",
      "Episode 2 | Reward: 2 | Policy Loss: [-0.17971687018871307, 0.02571047469973564, 0.025555619969964027, 0.02488286793231964] | Value Loss: [0.02502855658531189]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-30c9766906f4>:224: UserWarning: Using a target size (torch.Size([652])) that is different to the input size (torch.Size([163, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = F.mse_loss(self.value_network(all_obs), all_returns)\n",
      "<ipython-input-64-30c9766906f4>:231: UserWarning: Using a target size (torch.Size([652])) that is different to the input size (torch.Size([163, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = F.mse_loss(self.value_network(all_obs), all_returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Loss: 0.032777830958366394\n",
      "Policy Loss: 0.03323297202587128\n",
      "Policy Loss: 0.0327405110001564\n",
      "Policy Loss: 0.03632649779319763\n",
      "Value Loss: 0.0021832489874213934\n",
      "Policy Loss: 0.03273708000779152\n",
      "Policy Loss: 0.03319066762924194\n",
      "Policy Loss: 0.032671406865119934\n",
      "Policy Loss: 0.03632195666432381\n",
      "Value Loss: 0.0017422072123736143\n",
      "Policy Loss: 0.03268356993794441\n",
      "Policy Loss: 0.03313136845827103\n",
      "Policy Loss: 0.0325828455388546\n",
      "Policy Loss: 0.036249931901693344\n",
      "Value Loss: 0.0013027878012508154\n",
      "Policy Loss: 0.03261895105242729\n",
      "Policy Loss: 0.03305986896157265\n",
      "Policy Loss: 0.03248421475291252\n",
      "Policy Loss: 0.03613647073507309\n",
      "Value Loss: 0.0010138519573956728\n",
      "Policy Loss: 0.03254500776529312\n",
      "Policy Loss: 0.03297916427254677\n",
      "Policy Loss: 0.03238444775342941\n",
      "Policy Loss: 0.03599727526307106\n",
      "Value Loss: 0.000955459603574127\n",
      "Policy Loss: 0.03246358409523964\n",
      "Policy Loss: 0.032890331000089645\n",
      "Policy Loss: 0.0322830006480217\n",
      "Policy Loss: 0.03584461659193039\n",
      "Value Loss: 0.0010858842870220542\n",
      "Policy Loss: 0.0323764830827713\n",
      "Policy Loss: 0.03279731050133705\n",
      "Policy Loss: 0.032178014516830444\n",
      "Policy Loss: 0.03568604588508606\n",
      "Value Loss: 0.0012491518864408135\n",
      "Policy Loss: 0.032284967601299286\n",
      "Policy Loss: 0.03270091861486435\n",
      "Policy Loss: 0.03207147493958473\n",
      "Policy Loss: 0.03552756458520889\n",
      "Value Loss: 0.001330885454080999\n",
      "Policy Loss: 0.03218883275985718\n",
      "Policy Loss: 0.03260228410363197\n",
      "Policy Loss: 0.03196781501173973\n",
      "Policy Loss: 0.035367175936698914\n",
      "Value Loss: 0.0013067466206848621\n",
      "Policy Loss: 0.03208857774734497\n",
      "Policy Loss: 0.032501544803380966\n",
      "Policy Loss: 0.0318666510283947\n",
      "Policy Loss: 0.03521140292286873\n",
      "Value Loss: 0.0012142137857154012\n",
      "Episode 3 | Reward: 0 | Policy Loss: [0.03208857774734497, 0.032501544803380966, 0.0318666510283947, 0.03521140292286873] | Value Loss: [0.0012142137857154012]\n",
      "Policy Loss: -0.1835041493177414\n",
      "Policy Loss: 0.021676378324627876\n",
      "Policy Loss: -0.050405342131853104\n",
      "Policy Loss: 0.02573016658425331\n",
      "Value Loss: 0.059385720640420914\n",
      "Policy Loss: -0.18377937376499176\n",
      "Policy Loss: 0.02162765897810459\n",
      "Policy Loss: -0.05044950544834137\n",
      "Policy Loss: 0.02562856301665306\n",
      "Value Loss: 0.05908459797501564\n",
      "Policy Loss: -0.1841181367635727\n",
      "Policy Loss: 0.021570783108472824\n",
      "Policy Loss: -0.050504978746175766\n",
      "Policy Loss: 0.025512024760246277\n",
      "Value Loss: 0.058716073632240295\n",
      "Policy Loss: -0.18449413776397705\n",
      "Policy Loss: 0.02150588668882847\n",
      "Policy Loss: -0.0505676195025444\n",
      "Policy Loss: 0.025384090840816498\n",
      "Value Loss: 0.05836540833115578\n",
      "Policy Loss: -0.18490122258663177\n",
      "Policy Loss: 0.021437671035528183\n",
      "Policy Loss: -0.050636176019907\n",
      "Policy Loss: 0.025251355022192\n",
      "Value Loss: 0.05808848515152931\n",
      "Policy Loss: -0.18533150851726532\n",
      "Policy Loss: 0.021366499364376068\n",
      "Policy Loss: -0.050709277391433716\n",
      "Policy Loss: 0.02511879988014698\n",
      "Value Loss: 0.05792360007762909\n",
      "Policy Loss: -0.18577420711517334\n",
      "Policy Loss: 0.021293768659234047\n",
      "Policy Loss: -0.05078641325235367\n",
      "Policy Loss: 0.024986037984490395\n",
      "Value Loss: 0.05788465589284897\n",
      "Policy Loss: -0.18622463941574097\n",
      "Policy Loss: 0.021221090108156204\n",
      "Policy Loss: -0.05085897818207741\n",
      "Policy Loss: 0.024855641648173332\n",
      "Value Loss: 0.057950254529714584\n",
      "Policy Loss: -0.1866833120584488\n",
      "Policy Loss: 0.0211472250521183\n",
      "Policy Loss: -0.05092506855726242\n",
      "Policy Loss: 0.024731094017624855\n",
      "Value Loss: 0.058063607662916183\n",
      "Policy Loss: -0.18713010847568512\n",
      "Policy Loss: 0.02107672207057476\n",
      "Policy Loss: -0.05098415166139603\n",
      "Policy Loss: 0.02461477369070053\n",
      "Value Loss: 0.05815347656607628\n",
      "Episode 4 | Reward: 4 | Policy Loss: [-0.18713010847568512, 0.02107672207057476, -0.05098415166139603, 0.02461477369070053] | Value Loss: [0.05815347656607628]\n",
      "Policy Loss: 0.03342219442129135\n",
      "Policy Loss: -0.05795236676931381\n",
      "Policy Loss: 0.03132067248225212\n",
      "Policy Loss: 0.03137289732694626\n",
      "Value Loss: 0.013063600286841393\n",
      "Policy Loss: 0.03344154357910156\n",
      "Policy Loss: -0.05797958746552467\n",
      "Policy Loss: 0.03131774812936783\n",
      "Policy Loss: 0.031322091817855835\n",
      "Value Loss: 0.0130387581884861\n",
      "Policy Loss: 0.03340446949005127\n",
      "Policy Loss: -0.05804208666086197\n",
      "Policy Loss: 0.031287625432014465\n",
      "Policy Loss: 0.031238682568073273\n",
      "Value Loss: 0.012990030460059643\n",
      "Policy Loss: 0.033323533833026886\n",
      "Policy Loss: -0.058121420443058014\n",
      "Policy Loss: 0.031235726550221443\n",
      "Policy Loss: 0.031134910881519318\n",
      "Value Loss: 0.012938461266458035\n",
      "Policy Loss: 0.03321124613285065\n",
      "Policy Loss: -0.0582008957862854\n",
      "Policy Loss: 0.031165288761258125\n",
      "Policy Loss: 0.031020814552903175\n",
      "Value Loss: 0.01289613638073206\n",
      "Policy Loss: 0.03307566046714783\n",
      "Policy Loss: -0.05827024206519127\n",
      "Policy Loss: 0.031081680208444595\n",
      "Policy Loss: 0.030907008796930313\n",
      "Value Loss: 0.01286948099732399\n",
      "Policy Loss: 0.03292661905288696\n",
      "Policy Loss: -0.0583348385989666\n",
      "Policy Loss: 0.030992383137345314\n",
      "Policy Loss: 0.03080115094780922\n",
      "Value Loss: 0.012856301851570606\n",
      "Policy Loss: 0.03277363255620003\n",
      "Policy Loss: -0.05839414522051811\n",
      "Policy Loss: 0.030899258330464363\n",
      "Policy Loss: 0.03071369044482708\n",
      "Value Loss: 0.01285193208605051\n",
      "Policy Loss: 0.032618407160043716\n",
      "Policy Loss: -0.058454807847738266\n",
      "Policy Loss: 0.030804023146629333\n",
      "Policy Loss: 0.030642619356513023\n",
      "Value Loss: 0.012852078303694725\n",
      "Policy Loss: 0.03246628865599632\n",
      "Policy Loss: -0.05852225422859192\n",
      "Policy Loss: 0.030712375417351723\n",
      "Policy Loss: 0.03057858534157276\n",
      "Value Loss: 0.01285330019891262\n",
      "Episode 5 | Reward: 1 | Policy Loss: [0.03246628865599632, -0.05852225422859192, 0.030712375417351723, 0.03057858534157276] | Value Loss: [0.01285330019891262]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-30c9766906f4>:224: UserWarning: Using a target size (torch.Size([732])) that is different to the input size (torch.Size([183, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = F.mse_loss(self.value_network(all_obs), all_returns)\n",
      "<ipython-input-64-30c9766906f4>:231: UserWarning: Using a target size (torch.Size([732])) that is different to the input size (torch.Size([183, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = F.mse_loss(self.value_network(all_obs), all_returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Loss: -0.1520094871520996\n",
      "Policy Loss: -0.06301052868366241\n",
      "Policy Loss: 0.030346153303980827\n",
      "Policy Loss: 0.03055362030863762\n",
      "Value Loss: 0.03987071290612221\n",
      "Policy Loss: -0.15247133374214172\n",
      "Policy Loss: -0.06311887502670288\n",
      "Policy Loss: 0.03035547584295273\n",
      "Policy Loss: 0.030506698414683342\n",
      "Value Loss: 0.03966948762536049\n",
      "Policy Loss: -0.1530313938856125\n",
      "Policy Loss: -0.06325281411409378\n",
      "Policy Loss: 0.030335091054439545\n",
      "Policy Loss: 0.030442234128713608\n",
      "Value Loss: 0.039300158619880676\n",
      "Policy Loss: -0.15363198518753052\n",
      "Policy Loss: -0.06337764114141464\n",
      "Policy Loss: 0.030292289331555367\n",
      "Policy Loss: 0.030367789790034294\n",
      "Value Loss: 0.03888748213648796\n",
      "Policy Loss: -0.15419775247573853\n",
      "Policy Loss: -0.06348652392625809\n",
      "Policy Loss: 0.03023432195186615\n",
      "Policy Loss: 0.030294829979538918\n",
      "Value Loss: 0.038564566522836685\n",
      "Policy Loss: -0.15467382967472076\n",
      "Policy Loss: -0.063585065305233\n",
      "Policy Loss: 0.030166223645210266\n",
      "Policy Loss: 0.03022470511496067\n",
      "Value Loss: 0.03842587023973465\n",
      "Policy Loss: -0.15503785014152527\n",
      "Policy Loss: -0.06367643922567368\n",
      "Policy Loss: 0.030089491978287697\n",
      "Policy Loss: 0.030156750231981277\n",
      "Value Loss: 0.038491398096084595\n",
      "Policy Loss: -0.1552867293357849\n",
      "Policy Loss: -0.06376275420188904\n",
      "Policy Loss: 0.03001057542860508\n",
      "Policy Loss: 0.030089328065514565\n",
      "Value Loss: 0.03867777809500694\n",
      "Policy Loss: -0.15544623136520386\n",
      "Policy Loss: -0.0638442188501358\n",
      "Policy Loss: 0.029932381585240364\n",
      "Policy Loss: 0.030024511739611626\n",
      "Value Loss: 0.03883127123117447\n",
      "Policy Loss: -0.15555806457996368\n",
      "Policy Loss: -0.06393176317214966\n",
      "Policy Loss: 0.029854463413357735\n",
      "Policy Loss: 0.029962074011564255\n",
      "Value Loss: 0.038842007517814636\n",
      "Episode 6 | Reward: 3 | Policy Loss: [-0.15555806457996368, -0.06393176317214966, 0.029854463413357735, 0.029962074011564255] | Value Loss: [0.038842007517814636]\n",
      "Policy Loss: -0.1290343552827835\n",
      "Policy Loss: -0.043396495282649994\n",
      "Policy Loss: 0.03705639764666557\n",
      "Policy Loss: -0.044340599328279495\n",
      "Value Loss: 0.03890283778309822\n",
      "Policy Loss: -0.1292879730463028\n",
      "Policy Loss: -0.043401625007390976\n",
      "Policy Loss: 0.037049029022455215\n",
      "Policy Loss: -0.04434194415807724\n",
      "Value Loss: 0.038444116711616516\n",
      "Policy Loss: -0.1297530233860016\n",
      "Policy Loss: -0.043419815599918365\n",
      "Policy Loss: 0.03701217845082283\n",
      "Policy Loss: -0.04436219856142998\n",
      "Value Loss: 0.037760719656944275\n",
      "Policy Loss: -0.13029684126377106\n",
      "Policy Loss: -0.04344664141535759\n",
      "Policy Loss: 0.03695318102836609\n",
      "Policy Loss: -0.044396642595529556\n",
      "Value Loss: 0.037117406725883484\n",
      "Policy Loss: -0.13084106147289276\n",
      "Policy Loss: -0.043479785323143005\n",
      "Policy Loss: 0.03687388822436333\n",
      "Policy Loss: -0.044435761868953705\n",
      "Value Loss: 0.03671552985906601\n",
      "Policy Loss: -0.13131283223628998\n",
      "Policy Loss: -0.04351747781038284\n",
      "Policy Loss: 0.036778900772333145\n",
      "Policy Loss: -0.04448047652840614\n",
      "Value Loss: 0.03666340187191963\n",
      "Policy Loss: -0.13169507682323456\n",
      "Policy Loss: -0.04356083646416664\n",
      "Policy Loss: 0.03667391464114189\n",
      "Policy Loss: -0.044527359306812286\n",
      "Value Loss: 0.03690434247255325\n",
      "Policy Loss: -0.1319964975118637\n",
      "Policy Loss: -0.04360973834991455\n",
      "Policy Loss: 0.03656124323606491\n",
      "Policy Loss: -0.04457861930131912\n",
      "Value Loss: 0.03714105486869812\n",
      "Policy Loss: -0.13222019374370575\n",
      "Policy Loss: -0.04366299882531166\n",
      "Policy Loss: 0.0364382266998291\n",
      "Policy Loss: -0.04463214427232742\n",
      "Value Loss: 0.03711319714784622\n",
      "Policy Loss: -0.13240401446819305\n",
      "Policy Loss: -0.043719734996557236\n",
      "Policy Loss: 0.036306560039520264\n",
      "Policy Loss: -0.044686391949653625\n",
      "Value Loss: 0.03686830401420593\n",
      "Episode 7 | Reward: 4 | Policy Loss: [-0.13240401446819305, -0.043719734996557236, 0.036306560039520264, -0.044686391949653625] | Value Loss: [0.03686830401420593]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-30c9766906f4>:224: UserWarning: Using a target size (torch.Size([812])) that is different to the input size (torch.Size([203, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = F.mse_loss(self.value_network(all_obs), all_returns)\n",
      "<ipython-input-64-30c9766906f4>:231: UserWarning: Using a target size (torch.Size([812])) that is different to the input size (torch.Size([203, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = F.mse_loss(self.value_network(all_obs), all_returns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Loss: -0.2259960174560547\n",
      "Policy Loss: -0.13583824038505554\n",
      "Policy Loss: 0.05006949603557587\n",
      "Policy Loss: 0.04919302091002464\n",
      "Value Loss: 0.06244930252432823\n",
      "Policy Loss: -0.22633014619350433\n",
      "Policy Loss: -0.13605009019374847\n",
      "Policy Loss: 0.0498976893723011\n",
      "Policy Loss: 0.0491652749478817\n",
      "Value Loss: 0.06136344000697136\n",
      "Policy Loss: -0.22672267258167267\n",
      "Policy Loss: -0.13637715578079224\n",
      "Policy Loss: 0.04964287579059601\n",
      "Policy Loss: 0.049104299396276474\n",
      "Value Loss: 0.05960603803396225\n",
      "Policy Loss: -0.2271328568458557\n",
      "Policy Loss: -0.13676272332668304\n",
      "Policy Loss: 0.049343373626470566\n",
      "Policy Loss: 0.049025505781173706\n",
      "Value Loss: 0.058450426906347275\n",
      "Policy Loss: -0.22755901515483856\n",
      "Policy Loss: -0.1371614634990692\n",
      "Policy Loss: 0.04902255907654762\n",
      "Policy Loss: 0.04893675446510315\n",
      "Value Loss: 0.05845388025045395\n",
      "Policy Loss: -0.22796404361724854\n",
      "Policy Loss: -0.13755230605602264\n",
      "Policy Loss: 0.048710502684116364\n",
      "Policy Loss: 0.048851773142814636\n",
      "Value Loss: 0.05933017283678055\n",
      "Policy Loss: -0.22833989560604095\n",
      "Policy Loss: -0.13794194161891937\n",
      "Policy Loss: 0.04842598736286163\n",
      "Policy Loss: 0.048767585307359695\n",
      "Value Loss: 0.06001319736242294\n",
      "Policy Loss: -0.2287120372056961\n",
      "Policy Loss: -0.13835376501083374\n",
      "Policy Loss: 0.04818420857191086\n",
      "Policy Loss: 0.048683784902095795\n",
      "Value Loss: 0.05983578413724899\n",
      "Policy Loss: -0.22909177839756012\n",
      "Policy Loss: -0.1387835294008255\n",
      "Policy Loss: 0.04799572750926018\n",
      "Policy Loss: 0.048599421977996826\n",
      "Value Loss: 0.05911813676357269\n",
      "Policy Loss: -0.2294691801071167\n",
      "Policy Loss: -0.13919919729232788\n",
      "Policy Loss: 0.04787062481045723\n",
      "Policy Loss: 0.04852074384689331\n",
      "Value Loss: 0.0584920309484005\n",
      "Episode 8 | Reward: 5 | Policy Loss: [-0.2294691801071167, -0.13919919729232788, 0.04787062481045723, 0.04852074384689331] | Value Loss: [0.0584920309484005]\n",
      "Policy Loss: 0.0648469552397728\n",
      "Policy Loss: 0.06550238281488419\n",
      "Policy Loss: 0.06628476083278656\n",
      "Policy Loss: 0.06494779884815216\n",
      "Value Loss: 0.006159307900816202\n",
      "Policy Loss: 0.06482657045125961\n",
      "Policy Loss: 0.0656246691942215\n",
      "Policy Loss: 0.06601224094629288\n",
      "Policy Loss: 0.06490902602672577\n",
      "Value Loss: 0.003688960336148739\n",
      "Policy Loss: 0.06477668136358261\n",
      "Policy Loss: 0.06561864167451859\n",
      "Policy Loss: 0.06559198349714279\n",
      "Policy Loss: 0.06484314799308777\n",
      "Value Loss: 0.002120391698554158\n",
      "Policy Loss: 0.06470267474651337\n",
      "Policy Loss: 0.0655033215880394\n",
      "Policy Loss: 0.06511472910642624\n",
      "Policy Loss: 0.06475649774074554\n",
      "Value Loss: 0.0017681147437542677\n",
      "Policy Loss: 0.06460444629192352\n",
      "Policy Loss: 0.06528262794017792\n",
      "Policy Loss: 0.06462211161851883\n",
      "Policy Loss: 0.06465452909469604\n",
      "Value Loss: 0.0022514460142701864\n",
      "Policy Loss: 0.06448537111282349\n",
      "Policy Loss: 0.06498562544584274\n",
      "Policy Loss: 0.06414542347192764\n",
      "Policy Loss: 0.06455068290233612\n",
      "Value Loss: 0.002984466729685664\n",
      "Policy Loss: 0.06434512883424759\n",
      "Policy Loss: 0.06466330587863922\n",
      "Policy Loss: 0.06371354311704636\n",
      "Policy Loss: 0.06444444507360458\n",
      "Value Loss: 0.0035038108471781015\n",
      "Policy Loss: 0.0641893669962883\n",
      "Policy Loss: 0.06436949223279953\n",
      "Policy Loss: 0.06336048990488052\n",
      "Policy Loss: 0.06432551145553589\n",
      "Value Loss: 0.0036266541574150324\n",
      "Policy Loss: 0.06402397900819778\n",
      "Policy Loss: 0.06412319839000702\n",
      "Policy Loss: 0.06307001411914825\n",
      "Policy Loss: 0.06418921053409576\n",
      "Value Loss: 0.0034262388944625854\n",
      "Policy Loss: 0.06385153532028198\n",
      "Policy Loss: 0.0639127865433693\n",
      "Policy Loss: 0.0628313273191452\n",
      "Policy Loss: 0.0640372484922409\n",
      "Value Loss: 0.0030334594193845987\n",
      "Episode 9 | Reward: 0 | Policy Loss: [0.06385153532028198, 0.0639127865433693, 0.0628313273191452, 0.0640372484922409] | Value Loss: [0.0030334594193845987]\n",
      "Policy Loss: -0.3457339406013489\n",
      "Policy Loss: -0.2665659189224243\n",
      "Policy Loss: 0.03451305255293846\n",
      "Policy Loss: 0.033990971744060516\n",
      "Value Loss: 0.13198059797286987\n",
      "Policy Loss: -0.3462243378162384\n",
      "Policy Loss: -0.26686492562294006\n",
      "Policy Loss: 0.034488704055547714\n",
      "Policy Loss: 0.03398828208446503\n",
      "Value Loss: 0.12925510108470917\n",
      "Policy Loss: -0.3468326926231384\n",
      "Policy Loss: -0.26725533604621887\n",
      "Policy Loss: 0.03446460887789726\n",
      "Policy Loss: 0.03397645801305771\n",
      "Value Loss: 0.12555164098739624\n",
      "Policy Loss: -0.34733158349990845\n",
      "Policy Loss: -0.26765063405036926\n",
      "Policy Loss: 0.034438930451869965\n",
      "Policy Loss: 0.03395720571279526\n",
      "Value Loss: 0.12154898792505264\n",
      "Policy Loss: -0.3476673364639282\n",
      "Policy Loss: -0.2680255174636841\n",
      "Policy Loss: 0.03441188856959343\n",
      "Policy Loss: 0.033929746598005295\n",
      "Value Loss: 0.1177685409784317\n",
      "Policy Loss: -0.34789398312568665\n",
      "Policy Loss: -0.26835954189300537\n",
      "Policy Loss: 0.03438536077737808\n",
      "Policy Loss: 0.03389748930931091\n",
      "Value Loss: 0.11476235091686249\n",
      "Policy Loss: -0.34809020161628723\n",
      "Policy Loss: -0.2686530351638794\n",
      "Policy Loss: 0.03435877338051796\n",
      "Policy Loss: 0.03386112302541733\n",
      "Value Loss: 0.11346729844808578\n",
      "Policy Loss: -0.34831950068473816\n",
      "Policy Loss: -0.2689393162727356\n",
      "Policy Loss: 0.03433167189359665\n",
      "Policy Loss: 0.03382190689444542\n",
      "Value Loss: 0.11456334590911865\n",
      "Policy Loss: -0.3485913872718811\n",
      "Policy Loss: -0.26923108100891113\n",
      "Policy Loss: 0.034303274005651474\n",
      "Policy Loss: 0.03378155082464218\n",
      "Value Loss: 0.1168585941195488\n",
      "Policy Loss: -0.34888237714767456\n",
      "Policy Loss: -0.2695276737213135\n",
      "Policy Loss: 0.03427520766854286\n",
      "Policy Loss: 0.033740174025297165\n",
      "Value Loss: 0.11793413013219833\n",
      "Episode 10 | Reward: 18 | Policy Loss: [-0.34888237714767456, -0.2695276737213135, 0.03427520766854286, 0.033740174025297165] | Value Loss: [0.11793413013219833]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-30c9766906f4>:224: UserWarning: Using a target size (torch.Size([1772])) that is different to the input size (torch.Size([443, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = F.mse_loss(self.value_network(all_obs), all_returns)\n",
      "<ipython-input-64-30c9766906f4>:231: UserWarning: Using a target size (torch.Size([1772])) that is different to the input size (torch.Size([443, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = F.mse_loss(self.value_network(all_obs), all_returns)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from pettingzoo.butterfly import knights_archers_zombies_v10\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Create directory for checkpoints\n",
    "checkpoint_dir = \"mappo_kaz_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "def compute_gae(rewards, values, dones, gamma=0.99, lambda_=0.95):\n",
    "    \"\"\"\n",
    "    Compute Generalized Advantage Estimation (GAE).\n",
    "    \n",
    "    Args:\n",
    "        rewards: List of rewards for each time step.\n",
    "        values: List of value estimates for each time step.\n",
    "        dones: List of done flags for each time step.\n",
    "        gamma: Discount factor.\n",
    "        lambda_: GAE parameter.\n",
    "    \n",
    "    Returns:\n",
    "        advantages: List of GAE advantages.\n",
    "        returns: List of discounted returns.\n",
    "    \"\"\"\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    next_value = 0  # Assume zero value for the next state (or last state after done)\n",
    "    \n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + (1 - dones[step]) * gamma * next_value - values[step]\n",
    "        gae = delta + (1 - dones[step]) * gamma * lambda_ * gae\n",
    "        advantages.insert(0, gae)\n",
    "        next_value = values[step]\n",
    "        returns.insert(0, advantages[0] + values[step])\n",
    "    \n",
    "    advantages = torch.tensor(advantages, dtype=torch.float32)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    return advantages, returns\n",
    "\n",
    "def compute_policy_loss(new_log_probs, old_log_probs, advantages, clip_epsilon):\n",
    "    # Compute probability ratio\n",
    "    ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "    \n",
    "    # Calculate unclipped and clipped objectives\n",
    "    unclipped = ratio * advantages\n",
    "    clipped = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
    "    \n",
    "    # Take the minimum of the unclipped and clipped objectives\n",
    "    policy_loss = -torch.mean(torch.min(unclipped, clipped))\n",
    "    return policy_loss\n",
    "\n",
    "\n",
    "# Policy Network for each agent\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Centralized Value Network\n",
    "class CentralizedValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class MAPPO:\n",
    "    def __init__(self, env, lr=1e-3):\n",
    "        self.env = env\n",
    "        self.env.reset()  # Reset the environment to initialize agents\n",
    "        self.agents = env.agents\n",
    "        self.agent_policies = {}\n",
    "        self.optimizers = {}\n",
    "        \n",
    "        # Initialize separate policies and optimizers for each agent\n",
    "        for agent in self.agents:\n",
    "            input_dim = np.prod(env.observation_space(agent).shape)  # Flatten the observation space\n",
    "            output_dim = env.action_space(agent).n\n",
    "            policy = PolicyNetwork(input_dim, output_dim)\n",
    "            self.agent_policies[agent] = policy\n",
    "            self.optimizers[agent] = optim.Adam(policy.parameters(), lr=lr)\n",
    "        \n",
    "        # Centralized critic\n",
    "        centralized_input_dim = sum(\n",
    "            np.prod(env.observation_space(agent).shape) for agent in self.agents\n",
    "        )\n",
    "#         print('dim: ', centralized_input_dim) # 27 * 5 * 4 = 135*4=540\n",
    "        self.value_network = CentralizedValueNetwork(centralized_input_dim)\n",
    "        self.value_optimizer = optim.Adam(self.value_network.parameters(), lr=lr)\n",
    "\n",
    "    def train(self, num_episodes=2, gamma=0.99, lambda_=0.95, clip_epsilon=0.2, batch_size=64, update_epochs=10, checkpoint_interval=100):\n",
    "        rewards_per_episode = []\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            self.env.reset()\n",
    "            episode_rewards = {agent: 0 for agent in self.agents}\n",
    "            observations = {agent: None for agent in self.agents}\n",
    "            trajectories = {agent: {\"obs\": [], \"actions\": [], \"log_probs\": [], \"rewards\": [], \"values\": [], \"dones\": []} for agent in self.agents}\n",
    "                      \n",
    "            for agent in self.env.agent_iter():\n",
    "                observation, reward, termination, truncation, info = self.env.last()\n",
    "                done = termination or truncation\n",
    "                \n",
    "                # Convert observation to tensor and flatten it\n",
    "                obs_tensor = (\n",
    "                    torch.FloatTensor(observation).unsqueeze(0).view(1, -1)\n",
    "                    if observation is not None\n",
    "                    else torch.zeros(1, np.prod(self.env.observation_space(agent).shape))\n",
    "                )\n",
    "                \n",
    "                if not done:\n",
    "                \n",
    "                    # Get agent's policy and select an action\n",
    "                    action_probs = self.agent_policies[agent](obs_tensor)\n",
    "#                     print('ap1', action_probs)\n",
    "                    action = torch.multinomial(action_probs, 1).item()\n",
    "#                     print('a', action)\n",
    "\n",
    "                    #\n",
    "                    log_prob = torch.log(action_probs[0, action])\n",
    "                    value = self.value_network(torch.cat([trajectories[a][\"obs\"][-1] if trajectories[a][\"obs\"] else torch.zeros_like(obs_tensor) for a in self.agents], dim=-1))\n",
    "    \n",
    "                    # Store trajectory\n",
    "                    trajectories[agent][\"obs\"].append(obs_tensor)\n",
    "                    trajectories[agent][\"actions\"].append(action)\n",
    "                    trajectories[agent][\"log_probs\"].append(log_prob)\n",
    "                    trajectories[agent][\"values\"].append(value)\n",
    "                    trajectories[agent][\"rewards\"].append(reward)\n",
    "                    trajectories[agent][\"dones\"].append(done)\n",
    "                \n",
    "                    # Step the environment\n",
    "                    self.env.step(action)\n",
    "                    episode_rewards[agent] += reward\n",
    "                    observations[agent] = obs_tensor\n",
    "                    \n",
    "                else:\n",
    "                    self.env.step(None)  # No-op for terminated agent\n",
    "            \n",
    "            # Compute GAE and returns\n",
    "            for agent in self.agents:\n",
    "                advantages, returns = compute_gae(trajectories[agent][\"rewards\"],trajectories[agent][\"values\"],trajectories[agent][\"dones\"], gamma, lambda_)\n",
    "                trajectories[agent][\"advantages\"] = advantages\n",
    "                trajectories[agent][\"returns\"] = returns\n",
    "            \n",
    "            # PPO update\n",
    "            for _ in range(update_epochs):\n",
    "                all_obs = []\n",
    "                all_returns = []\n",
    "                \n",
    "                policy_losses = []\n",
    "                value_losses = []\n",
    "                \n",
    "                # Each episode has different number of steps due to termination, so we will pad each episode to max length\n",
    "                # , then mask the padded steps later\n",
    "                max_steps = max(len(trajectories[agent][\"obs\"]) for agent in self.agents)\n",
    "                \n",
    "                for agent in self.agents:\n",
    "                    obs = torch.cat(trajectories[agent][\"obs\"], dim=0)\n",
    "                    actions = torch.tensor(trajectories[agent][\"actions\"])\n",
    "                    old_log_probs = torch.tensor(trajectories[agent][\"log_probs\"])\n",
    "                    advantages = torch.tensor(trajectories[agent][\"advantages\"])\n",
    "                    returns = torch.tensor(trajectories[agent][\"returns\"])\n",
    "                    \n",
    "#                     print(returns.shape)\n",
    "                    # Pad obs and returns to match the longest trajectory\n",
    "                    padding_obs = max_steps - obs.shape[0]\n",
    "                    if padding_obs > 0:\n",
    "                        obs = F.pad(obs, (0, 0, 0, padding_obs))  # Pad time steps (dim=0), no padding for features (dim=1)\n",
    "                    if padding_obs > 0:\n",
    "                        returns = F.pad(returns, (0, padding_obs))  # Pad time steps (1D tensor)\n",
    "    \n",
    "                    \n",
    "                    action_probs = self.agent_policies[agent](obs)\n",
    "#                     print('ap2', action_probs)\n",
    "                    new_log_probs = torch.log(action_probs.gather(1, actions.unsqueeze(-1)))\n",
    "                    # policy needs to be updated for each agent\n",
    "                    policy_loss = compute_policy_loss(new_log_probs, old_log_probs, advantages, clip_epsilon)\n",
    "                    \n",
    "                    \n",
    "                    # Calculate accuracy for the policy network\n",
    "                    policy_losses.append(policy_loss.item())\n",
    "#                     print(f\"Policy Loss:\", policy_loss.item())\n",
    "                    \n",
    "                    # value needs to be updated for all agents, so we store each obs for process later\n",
    "                    all_obs.append(obs)\n",
    "                    all_returns.append(returns)\n",
    "                    \n",
    "                    # update policy for each agent\n",
    "                    self.optimizers[agent].zero_grad()\n",
    "                    policy_loss.backward()\n",
    "                    self.optimizers[agent].step()\n",
    "                \n",
    "                # update value for all agents\n",
    "                all_obs = torch.cat(all_obs, dim=-1)\n",
    "                all_returns = torch.cat(all_returns, dim=-1)\n",
    "                \n",
    "                value_loss = F.mse_loss(self.value_network(all_obs), all_returns)\n",
    "                value_losses.append(value_loss.item())\n",
    "                print(f\"Value Loss: {value_loss.item()}\")\n",
    "\n",
    "                # Mask padding in value loss calculation\n",
    "                mask = torch.cat([torch.ones(len(trajectories[agent][\"obs\"])) for agent in self.agents])\n",
    "                mask = F.pad(mask, (0, max_steps * len(self.agents) - len(mask))).bool()\n",
    "                value_loss = F.mse_loss(self.value_network(all_obs), all_returns)\n",
    "                self.value_optimizer.zero_grad()\n",
    "                value_loss.backward()\n",
    "                self.value_optimizer.step()\n",
    "            \n",
    "            # Total episode reward\n",
    "            total_episode_reward = sum(episode_rewards.values())\n",
    "            rewards_per_episode.append(total_episode_reward)\n",
    "            \n",
    "#             avg_policy_loss = np.mean(policy_losses)\n",
    "#             avg_value_loss = np.mean(value_losses)\n",
    "\n",
    "#             print(f\"Episode {episode + 1} | Reward: {total_episode_reward} | Policy Loss: {policy_losses} | Value Loss: {value_losses}\")\n",
    "\n",
    "            \n",
    "            if (episode + 1) % 100 == 0:\n",
    "                print(f\"Episode {episode+1} total reward: {total_episode_reward}\")\n",
    "            \n",
    "            if (episode + 1) % checkpoint_interval == 0:\n",
    "                checkpoint_path = os.path.join(checkpoint_dir, f'mappo_kaz_checkpoint_{episode + 1}.pth')\n",
    "                torch.save({\n",
    "                    'agent_policies': {agent: self.agent_policies[agent].state_dict() for agent in self.agents},\n",
    "                    'value_network_state_dict': self.value_network.state_dict(),\n",
    "                }, checkpoint_path)\n",
    "                print(f\"Checkpoint saved at episode {episode + 1}\")\n",
    "        \n",
    "        return rewards_per_episode\n",
    "\n",
    "\n",
    "\n",
    "def plot_rewards(rewards):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards)\n",
    "    plt.title('Rewards per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(checkpoint_dir, 'kaz_rewards.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Initialize environment\n",
    "env = knights_archers_zombies_v10.env()\n",
    "\n",
    "# Create and train MAPPO agent\n",
    "mappo_agent = MAPPO(env)\n",
    "rewards = mappo_agent.train(num_episodes=10000)\n",
    "\n",
    "# Plot rewards\n",
    "plot_rewards(rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5acfc7",
   "metadata": {},
   "source": [
    "We can see the reward is trending upwards, indicating that the model is learning. \n",
    "\n",
    "Another way to debug is by checking the loss to see if the neural network (NN) is learning from the data. The value loss remains positive and not too large, which is good since Mean Squared Error (MSE) should always be positive. The policy loss, influenced by the PPO clipping method, can sometimes be negative. While small negative values are good, values around 0.27 are questionable but not disastrous.\n",
    "\n",
    "Given that training Multi-Agent Reinforcement Learning (MARL) agents is resource-intensive, we save checkpoints to guard against crashes and facilitate re-training from a good checkpoint if needed. Additionally, sometimes an agent performs well initially but degrades later, so we can revert to the best-performing checkpoint.\n",
    "\n",
    "We will now load a saved checkpoint, run an episode, and record a video to observe the agent's performance. Seeing the agent in action provides deeper insights into its behavior and learning outcomes, beyond just looking at rewards. This is important to ensure it's not just gaming the system but performing its intended task effectively (referencing the RL danger slide where high rewards might indicate system manipulation rather than proper task execution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab872750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_episode(checkpoint_path):\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from torch.nn.functional import softmax\n",
    "\n",
    "    # Initialize environment with rendering\n",
    "    env = knights_archers_zombies_v10.env(render_mode='human')\n",
    "    env.reset()\n",
    "    \n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    # Get agents and initialize their policies\n",
    "    agents = env.agents\n",
    "    agent_policies = {}\n",
    "    for agent in agents:\n",
    "        # Our policy is of shape: agents * 27 * 5, since it is trained via centralized critic\n",
    "        # The env obs shape is 27 * 5, so we need to reshape the input_dim to collect all obs from all agents \n",
    "        # and sum them up to the correct dim\n",
    "        input_dim = int(np.prod(env.observation_space(agent).shape))\n",
    "        output_dim = env.action_space(agent).n\n",
    "\n",
    "        # Create and load the policy network for the agent\n",
    "        policy_network = PolicyNetwork(input_dim, output_dim)\n",
    "        policy_network.load_state_dict(checkpoint['agent_policies'][agent])\n",
    "        policy_network.eval()\n",
    "        agent_policies[agent] = policy_network\n",
    "    \n",
    "    # Play one episode\n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        \n",
    "        if termination or truncation:\n",
    "            env.step(None)\n",
    "            continue\n",
    "        \n",
    "        # Flatten the observation and convert it to a tensor\n",
    "        obs_tensor = torch.FloatTensor(observation).view(1, -1)\n",
    "        \n",
    "        # Select action using the agent's policy\n",
    "        with torch.no_grad():\n",
    "            action_logits = agent_policies[agent](obs_tensor)\n",
    "            action_probs = softmax(action_logits, dim=-1)\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "        \n",
    "        # Take the action in the environment\n",
    "        env.step(action)\n",
    "    \n",
    "    # Close the environment after the episode ends\n",
    "    env.close()\n",
    "    print(\"Episode rendered successfully.\")\n",
    "\n",
    "render_episode('mappo_kaz_checkpoint_20300.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2987cdaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
