{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d74f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pettingzoo \n",
    "# !pip install pygame \n",
    "# !pip install rlcard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a3ffdb",
   "metadata": {},
   "source": [
    "Leduc Poker:\n",
    "https://pettingzoo.farama.org/environments/classic/leduc_holdem/\n",
    "\n",
    "Please see this site for rules and observation space and action space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a991748",
   "metadata": {},
   "source": [
    "CFR:\n",
    "- strategy_sum is a data structure (typically a dictionary) that accumulates the strategies used by the player for each game state across many iterations of the algorithm.\n",
    "- regret_sum in CFR accumulates the regrets with respect to the strategies chosen over the iterations\n",
    "- regreats is utility - Expected utility: regrets[action] += utility(action) - expected_utility over all actions\n",
    "- The strategy is similar to a policy, so cfr_player.get_average_strategy(state) will give you a probability of actions given the state.\n",
    "    - For example: \n",
    "        - actual_utility = 3 # Received for taking 'call'\n",
    "        - counterfactual_utilities = [4 (if raised), 3 (if called), 1 (if folded)]\n",
    "        - regrets[0] = (4 - 3)  # Regret for not raising\n",
    "        - regrets[1] = (3 - 3)  # Regret for not calling (no regret)\n",
    "        - regrets[2] = (1 - 3)  # Regret for not folding\n",
    "        - regreat[a] = 3 - (p(0)*4 + p(1)*3 + p(2)*(-2))\n",
    "    - Not saying this is the best regret function, but it is one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891c443c",
   "metadata": {},
   "source": [
    "# Random Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aab43b",
   "metadata": {},
   "source": [
    "Note that the environment provides an action mask, which we should utilize. If an agent takes an illegal move, it immediately loses the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "522ee007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 {'observation': array([1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.], dtype=float32), 'action_mask': array([1, 1, 1, 0], dtype=int8)} 0\n",
      "1\n",
      "player_0 {'observation': array([0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.], dtype=float32), 'action_mask': array([1, 1, 1, 0], dtype=int8)} 0\n",
      "0\n",
      "player_1 {'observation': array([1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.], dtype=float32), 'action_mask': array([0, 1, 1, 1], dtype=int8)} 0\n",
      "3\n",
      "player_0 {'observation': array([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.], dtype=float32), 'action_mask': array([0, 1, 1, 1], dtype=int8)} 0\n",
      "1\n",
      "player_1 {'observation': array([1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0.], dtype=float32), 'action_mask': array([1, 1, 1, 0], dtype=int8)} 0\n",
      "1\n",
      "player_0 {'observation': array([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0.], dtype=float32), 'action_mask': array([1, 0, 1, 0], dtype=int8)} 0\n",
      "0\n",
      "player_0 {'observation': array([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0.], dtype=float32), 'action_mask': array([0, 0, 0, 0], dtype=int8)} -6.0\n",
      "player_1 {'observation': array([1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0.], dtype=float32), 'action_mask': array([0, 0, 0, 0], dtype=int8)} 6.0\n",
      "{'observation': array([1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0.], dtype=float32), 'action_mask': array([0, 0, 0, 0], dtype=int8)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pettingzoo.classic import leduc_holdem_v4\n",
    "import random\n",
    "\n",
    "# Create the Leduc Hold'em environment\n",
    "env = leduc_holdem_v4.env()\n",
    "env.reset()\n",
    "\n",
    "def get_legal_action(action_space, action_mask): \n",
    "    legal_actions = [action for action, mask in enumerate(action_mask) if mask == 1] \n",
    "    return legal_actions\n",
    "\n",
    "# Perform random actions in the environment\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, done, truncation, info = env.last()\n",
    "    print(agent, observation, reward)\n",
    "    if done:\n",
    "        action = None\n",
    "    else:\n",
    "        action_mask = observation['action_mask']\n",
    "        action_mask[2] = 0\n",
    "        action = random.choice(get_legal_action(env.action_space(agent), action_mask))\n",
    "        print(action)\n",
    "    env.step(action)\n",
    "    env.render()\n",
    "\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca9d31",
   "metadata": {},
   "source": [
    "# Player_0 is CFR; player_1 is random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391bcb7b",
   "metadata": {},
   "source": [
    "To calculate regret, we need to model the opponent's behavior. We don't have a highly skilled or adversarial agent available, so given the simplicity of Leduc Poker, it is reasonable to assume a random agent as the opponent (not ideal, but sufficient for our purposes). Naturally, both players will use the action mask.\n",
    "\n",
    "Let's see if our player can discover the optimal strategy or reach a Nash equilibrium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01b397f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.29376774 0.31332407 0.34333796 0.04957022]\n",
      "State (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.29469347 0.3125     0.34375    0.04905653]\n",
      "State (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.42206727 0.07013946 0.46493027 0.042863  ]\n",
      "State (0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.40445026 0.19109948 0.40445026 0.        ]\n",
      "State (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.30668842 0.29395505 0.35302248 0.04633406]\n",
      "State (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.30751379 0.29241222 0.35379389 0.0462801 ]\n",
      "State (0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.08256881 0.31039755 0.34480122 0.26223242]\n",
      "State (0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.29387146 0.31360988 0.34319506 0.0493236 ]\n",
      "State (0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.41390254 0.07418782 0.46290609 0.04900355]\n",
      "State (0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.3624197 0.2751606 0.3624197 0.       ]\n",
      "State (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.07770129 0.31101377 0.34449312 0.26679182]\n",
      "State (0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.07851852 0.30962963 0.34518519 0.26666667]\n",
      "State (0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.30837256 0.29327505 0.35336247 0.04498991]\n",
      "State (0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.35859107 0.28281787 0.35859107 0.        ]\n",
      "State (1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.36225266 0.27549467 0.36225266 0.        ]\n",
      "State (0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.08202989 0.31004519 0.34497741 0.26294751]\n",
      "State (0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.0872449  0.30782313 0.34608844 0.25884354]\n",
      "State (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.07445258 0.31170415 0.34414792 0.26969535]\n",
      "State (0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.07558516 0.31172077 0.34413961 0.26855446]\n",
      "State (0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.3625 0.275  0.3625 0.    ]\n",
      "State (0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.4068915  0.18621701 0.4068915  0.        ]\n",
      "State (0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.07717475 0.31113677 0.34443161 0.26725686]\n",
      "State (0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.36316872 0.27366255 0.36316872 0.        ]\n",
      "State (1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.08481262 0.3086785  0.34566075 0.26084813]\n",
      "State (1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.40662651 0.18674699 0.40662651 0.        ]\n",
      "State (0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.07966072 0.31252186 0.34373907 0.26407835]\n",
      "State (0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.08114657 0.30933806 0.34533097 0.2641844 ]\n",
      "State (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.41744076 0.07209208 0.46395396 0.04651319]\n",
      "State (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.40767045 0.18465909 0.40767045 0.        ]\n",
      "State (0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.08333333 0.31028369 0.34485816 0.26152482]\n",
      "State (0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0): Average strategy [0.5 0.  0.5 0. ]\n",
      "State (1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.07928546 0.30887012 0.34556494 0.26627948]\n",
      "State (1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.08615819 0.30945044 0.34527478 0.25911659]\n",
      "State (1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.40770349 0.18459302 0.40770349 0.        ]\n",
      "State (0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.40433925 0.1913215  0.40433925 0.        ]\n",
      "State (1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0): Average strategy [0.5 0.  0.5 0. ]\n",
      "State (0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.08112324 0.31045242 0.34477379 0.26365055]\n",
      "State (1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.36013302 0.27973396 0.36013302 0.        ]\n",
      "State (0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.36219419 0.27561162 0.36219419 0.        ]\n",
      "State (0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0): Average strategy [0.5 0.  0.5 0. ]\n",
      "State (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.08298719 0.30823814 0.34588093 0.26289373]\n",
      "State (0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.07860644 0.31022409 0.34488796 0.26628151]\n",
      "State (1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.08612975 0.31021626 0.34489187 0.25876212]\n",
      "State (0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.0748227  0.30992908 0.34503546 0.27021277]\n",
      "State (1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.08256292 0.30970724 0.34514638 0.26258346]\n",
      "State (0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.0877451  0.30539216 0.34730392 0.25955882]\n",
      "State (0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.35897436 0.28205128 0.35897436 0.        ]\n",
      "State (0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.08108542 0.30956969 0.34521516 0.26412974]\n",
      "State (0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.3628003 0.2743994 0.3628003 0.       ]\n",
      "State (1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.08320473 0.30838477 0.34580761 0.26260288]\n",
      "State (1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.36127895 0.27744209 0.36127895 0.        ]\n",
      "State (0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0): Average strategy [0.5 0.  0.5 0. ]\n",
      "State (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.36358447 0.27283105 0.36358447 0.        ]\n",
      "State (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.07722348 0.31090955 0.34454523 0.26732174]\n",
      "State (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.07806882 0.31124807 0.34437596 0.26630714]\n",
      "State (0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.07978723 0.31173436 0.34413282 0.26434558]\n",
      "State (1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0): Average strategy [0.5 0.  0.5 0. ]\n",
      "State (0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.40873016 0.18253968 0.40873016 0.        ]\n",
      "State (0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.36111111 0.27777778 0.36111111 0.        ]\n",
      "State (1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0): Average strategy [0.5 0.  0.5 0. ]\n",
      "State (0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.08181588 0.30778958 0.34610521 0.26428933]\n",
      "State (0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.08346375 0.30829421 0.3458529  0.26238915]\n",
      "State (0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0): Average strategy [0.5 0.  0.5 0. ]\n",
      "State (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.36036036 0.27927928 0.36036036 0.        ]\n",
      "State (0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.36316288 0.27367424 0.36316288 0.        ]\n",
      "State (0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0): Average strategy [0.5 0.  0.5 0. ]\n",
      "State (0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.07881356 0.31468927 0.34265537 0.26384181]\n",
      "State (0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.36345967 0.27308066 0.36345967 0.        ]\n",
      "State (0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.40440357 0.19119287 0.40440357 0.        ]\n",
      "State (0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0): Average strategy [0.5 0.  0.5 0. ]\n",
      "State (0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0): Average strategy [0.5 0.  0.5 0. ]\n",
      "State (0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.35828877 0.28342246 0.35828877 0.        ]\n",
      "State (1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.3627451 0.2745098 0.3627451 0.       ]\n",
      "State (1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0): Average strategy [0.5 0.  0.5 0. ]\n",
      "State (0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0): Average strategy [0.5 0.  0.5 0. ]\n",
      "State (0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.08735632 0.30689655 0.34655172 0.2591954 ]\n",
      "State (0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.36069024 0.27861953 0.36069024 0.        ]\n",
      "State (0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.40462428 0.19075145 0.40462428 0.        ]\n",
      "State (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0): Average strategy [0.5 0.  0.5 0. ]\n",
      "State (0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0): Average strategy [0.5 0.  0.5 0. ]\n",
      "State (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0): Average strategy [0.5 0.  0.5 0. ]\n",
      "State (0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0): Average strategy [0.5 0.  0.5 0. ]\n",
      "State (0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0): Average strategy [0.5 0.  0.5 0. ]\n",
      "State (0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0): Average strategy [0.5 0.  0.5 0. ]\n"
     ]
    }
   ],
   "source": [
    "class CFRPlayer:\n",
    "    def __init__(self):\n",
    "        self.regret_sum = defaultdict(lambda: np.zeros(4))  # Regrets for four possible actions\n",
    "        self.strategy_sum = defaultdict(lambda: np.zeros(4))  # Strategy for four actions\n",
    "\n",
    "    def get_strategy(self, state, action_mask):\n",
    "        # Get the cumulative regrets for the state\n",
    "        regrets = self.regret_sum[state]\n",
    "        \n",
    "        # Apply regret matching only on legal actions\n",
    "        positive_regrets = np.maximum(regrets, 0) * action_mask  # Mask out illegal actions\n",
    "        normalizing_sum = np.sum(positive_regrets)\n",
    "        \n",
    "        # Define the strategy, normalized over legal actions\n",
    "        strategy = (\n",
    "            positive_regrets / normalizing_sum if normalizing_sum > 0 else action_mask / np.sum(action_mask)\n",
    "        )\n",
    "        \n",
    "        # Track strategy for this state for averaging\n",
    "        self.strategy_sum[state] += strategy\n",
    "        return strategy\n",
    "\n",
    "    def get_action(self, state, action_mask):\n",
    "        strategy = self.get_strategy(state, action_mask)\n",
    "        return np.random.choice(len(action_mask), p=strategy)  # Chooses based on masked strategy\n",
    "\n",
    "    def update_regrets(self, state, action, utility, expected_utility):\n",
    "        regrets = self.regret_sum[state]\n",
    "        regrets[action] += utility - expected_utility\n",
    "\n",
    "    def get_average_strategy(self, state):\n",
    "        strategy_sum = self.strategy_sum[state]\n",
    "        normalizing_sum = np.sum(strategy_sum)\n",
    "        return (\n",
    "            strategy_sum / normalizing_sum if normalizing_sum > 0 else np.ones(4) / 4\n",
    "        )\n",
    "\n",
    "# Updated CFR Simulation with Action Mask\n",
    "def cfr_iteration(env, cfr_player):\n",
    "    state_action_utility = {}\n",
    "    env.reset()\n",
    "\n",
    "    # Each player takes actions\n",
    "    for agent in env.agent_iter():\n",
    "        obs, reward, done, _, _ = env.last()  # Adjusted to unpack only three values\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        action_mask = obs['action_mask']  # Get the action mask\n",
    "        state = tuple(obs['observation'])  # State representation\n",
    "\n",
    "        if agent == \"player_0\":  # CFR Player\n",
    "            strategy = cfr_player.get_strategy(state, action_mask)\n",
    "            action = np.random.choice(len(action_mask), p=strategy)\n",
    "            env.step(action)\n",
    "            # Store utilities and regrets after the fact\n",
    "            state_action_utility[(state, action)] = reward\n",
    "\n",
    "        else:  # player_1 (Random)\n",
    "            legal_actions = [i for i, legal in enumerate(action_mask) if legal]\n",
    "            action = random.choice(legal_actions)\n",
    "            env.step(action)\n",
    "\n",
    "    # Update CFR regrets\n",
    "    for (state, action), utility in state_action_utility.items():\n",
    "        expected_utility = sum(\n",
    "            prob * state_action_utility.get((state, act), 0)\n",
    "            for act, prob in enumerate(cfr_player.get_strategy(state, action_mask))\n",
    "        )\n",
    "        cfr_player.update_regrets(state, action, utility, expected_utility)\n",
    "        \n",
    "# Run CFR on Leduc Poker\n",
    "num_iterations = 1000\n",
    "cfr_player = CFRPlayer()\n",
    "env = leduc_holdem_v4.env()\n",
    "for _ in range(num_iterations):\n",
    "    cfr_iteration(env, cfr_player)\n",
    "\n",
    "# Show final strategy\n",
    "for state in cfr_player.strategy_sum:\n",
    "    print(f\"State {state}: Average strategy {cfr_player.get_average_strategy(state)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc84968",
   "metadata": {},
   "source": [
    "# Player_0 is CFR; player_1 always calls (if legal, otherwise fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50630961",
   "metadata": {},
   "source": [
    "Lets see if we change the opponent strategy, how does that effect our strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba5c81bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.01910828 0.33333333 0.33333333 0.31422505]\n",
      "State (0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.01886792 0.33333333 0.33333333 0.31446541]\n",
      "State (0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.03985507 0.33333333 0.33333333 0.29347826]\n",
      "State (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.01364522 0.33333333 0.33333333 0.31968811]\n",
      "State (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.24267399 0.33333333 0.33333333 0.09065934]\n",
      "State (0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.24157303 0.33333333 0.33333333 0.0917603 ]\n",
      "State (0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.05263158 0.33333333 0.33333333 0.28070175]\n",
      "State (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.23965142 0.33333333 0.33333333 0.09368192]\n",
      "State (1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.05555556 0.33333333 0.33333333 0.27777778]\n",
      "State (0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.03174603 0.33333333 0.33333333 0.3015873 ]\n",
      "State (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.04954955 0.33333333 0.33333333 0.28378378]\n",
      "State (0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.06802721 0.33333333 0.33333333 0.26530612]\n",
      "State (0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.04402516 0.33333333 0.33333333 0.28930818]\n",
      "State (0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.03508772 0.33333333 0.33333333 0.29824561]\n",
      "State (1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0): Average strategy [0.05208333 0.33333333 0.33333333 0.28125   ]\n"
     ]
    }
   ],
   "source": [
    "class CFRPlayer:\n",
    "    def __init__(self):\n",
    "        self.regret_sum = defaultdict(lambda: np.zeros(4))  # Regrets for four possible actions\n",
    "        self.strategy_sum = defaultdict(lambda: np.zeros(4))  # Strategy for four actions\n",
    "\n",
    "    def get_strategy(self, state, action_mask):\n",
    "        # Get the cumulative regrets for the state\n",
    "        regrets = self.regret_sum[state]\n",
    "        \n",
    "        # Apply regret matching only on legal actions\n",
    "        positive_regrets = np.maximum(regrets, 0) * action_mask  # Mask out illegal actions\n",
    "        normalizing_sum = np.sum(positive_regrets)\n",
    "        \n",
    "        # Define the strategy, normalized over legal actions\n",
    "        strategy = (\n",
    "            positive_regrets / normalizing_sum if normalizing_sum > 0 else action_mask / np.sum(action_mask)\n",
    "        )\n",
    "        \n",
    "        # Track strategy for this state for averaging\n",
    "        self.strategy_sum[state] += strategy\n",
    "        return strategy\n",
    "\n",
    "    def get_action(self, state, action_mask):\n",
    "        strategy = self.get_strategy(state, action_mask)\n",
    "        return np.random.choice(len(action_mask), p=strategy)  # Chooses based on masked strategy\n",
    "\n",
    "    def update_regrets(self, state, action, utility, expected_utility):\n",
    "        regrets = self.regret_sum[state]\n",
    "        regrets[action] += utility - expected_utility\n",
    "\n",
    "    def get_average_strategy(self, state):\n",
    "        strategy_sum = self.strategy_sum[state]\n",
    "        normalizing_sum = np.sum(strategy_sum)\n",
    "        return (\n",
    "            strategy_sum / normalizing_sum if normalizing_sum > 0 else np.ones(4) / 4\n",
    "        )\n",
    "\n",
    "\n",
    "def cfr_iteration(env, cfr_player):\n",
    "    state_action_utility = {}\n",
    "    env.reset()\n",
    "\n",
    "    # Each player takes actions\n",
    "    for agent in env.agent_iter():\n",
    "        obs, reward, done, _, _ = env.last()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        action_mask = obs['action_mask']\n",
    "        state = tuple(obs['observation'])\n",
    "\n",
    "        if agent == \"player_0\":  # CFR Player\n",
    "            strategy = cfr_player.get_strategy(state, action_mask)\n",
    "            action = np.random.choice(len(action_mask), p=strategy)\n",
    "            env.step(action)\n",
    "            # Store utilities and regrets after the fact\n",
    "            state_action_utility[(state, action)] = reward\n",
    "\n",
    "        else: # player_1 (Call or Fold) \n",
    "            if action_mask[0]: # Check if action 0 (call) is legal \n",
    "                action = 0 \n",
    "            else: \n",
    "                action = 2\n",
    "            env.step(action)\n",
    "\n",
    "    # Update CFR regrets\n",
    "    for (state, action), utility in state_action_utility.items():\n",
    "        expected_utility = sum(\n",
    "            prob * state_action_utility.get((state, act), 0)\n",
    "            for act, prob in enumerate(cfr_player.get_strategy(state, action_mask))\n",
    "        )\n",
    "        cfr_player.update_regrets(state, action, utility, expected_utility)\n",
    "        \n",
    "# Run CFR on Leduc Poker\n",
    "num_iterations = 1000\n",
    "cfr_player = CFRPlayer()\n",
    "env = leduc_holdem_v4.env()\n",
    "for _ in range(num_iterations):\n",
    "    cfr_iteration(env, cfr_player)\n",
    "\n",
    "# Show final strategy\n",
    "for state in cfr_player.strategy_sum:\n",
    "    print(f\"State {state}: Average strategy {cfr_player.get_average_strategy(state)}\")\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676cb068",
   "metadata": {},
   "source": [
    "# Play game against random agent using above strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b4331820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for player_0 over 1000 games: 0.0245\n"
     ]
    }
   ],
   "source": [
    "def play_games_with_avg_strategy(cfr_player, num_games):\n",
    "    results = []\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        env = leduc_holdem_v4.env()\n",
    "        env.reset()\n",
    "        game_rewards = {\"player_0\": 0, \"player_1\": 0}\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "            done = termination or truncation\n",
    "            action_mask = observation['action_mask']\n",
    "            state = tuple(observation['observation'])\n",
    "\n",
    "            if done:\n",
    "                game_rewards[agent] += reward\n",
    "                env.step(None)  # Ensure we call step with None for dead agents\n",
    "            else:\n",
    "                if agent == \"player_0\":  # CFR Player with average strategy\n",
    "                    strategy = cfr_player.get_average_strategy(state)\n",
    "                    legal_actions = [i for i, legal in enumerate(action_mask) if legal]\n",
    "                    legal_strategy = [strategy[i] for i in legal_actions]\n",
    "                    legal_strategy = legal_strategy / np.sum(legal_strategy)  # Normalize\n",
    "                    action = np.random.choice(legal_actions, p=legal_strategy)\n",
    "                    env.step(action)\n",
    "                else:  # Random player\n",
    "                    legal_actions = [i for i, legal in enumerate(action_mask) if legal]\n",
    "                    action = random.choice(legal_actions)\n",
    "                    env.step(action)\n",
    "\n",
    "            env.render()\n",
    "\n",
    "        # Record the final reward for player_0\n",
    "        results.append(game_rewards[\"player_0\"])\n",
    "        env.close()  # Close the environment after each game\n",
    "\n",
    "    average_reward = np.mean(results)\n",
    "    print(f\"Average reward for player_0 over {num_games} games: {average_reward}\")\n",
    "\n",
    "# Run the evaluation phase\n",
    "num_games = 1000\n",
    "play_games_with_avg_strategy(cfr_player, num_games)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460cb2bf",
   "metadata": {},
   "source": [
    "It seems we win more games than we lose, which is promising considering the stochastic, partially observable, and adversarial nature of the game.\n",
    "\n",
    "One might ask: If Counterfactual Regret Minimization (CFR) can handle stochastic, partially observable, adversarial games, why not apply it to games like Minesweeper and treat the environment as the adversary? There's no clear-cut answer since each problem has its unique challenges. \n",
    "\n",
    "While CFR is used in real poker with substantial resources, sometimes leveraging deep learning. But Calculating a comprehensive strategy for complex games often demands more resources than I can manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a79cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
